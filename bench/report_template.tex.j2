%% report_template.tex.j2 -- Jinja2 LaTeX template for the matryoshka
%% comparative benchmark report.
%%
%% Custom Jinja2 delimiters for LaTeX compatibility:
%%   BLOCK, VAR, and comment delimiters with backslash prefix.
%%
\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=matryoshkacolor,urlcolor=matryoshkacolor,citecolor=matryoshkacolor]{hyperref}
\usepackage{fancyhdr}
\usepackage{siunitx}
\usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{float}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{listings}
\usepackage{caption}
\usepackage{colortbl}

%% ── Colours ─────────────────────────────────────────────────────────────────
\definecolor{matryoshkacolor}{RGB}{31,119,180}
\definecolor{stdsetcolor}{RGB}{214,39,40}
\definecolor{tlxcolor}{RGB}{148,103,189}
\definecolor{artcolor}{RGB}{255,127,14}
\definecolor{abseilcolor}{RGB}{44,160,44}
\definecolor{rowhl}{RGB}{220,230,242}

%% ── Page style ──────────────────────────────────────────────────────────────
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Matryoshka B+ Tree Benchmark Report}
\fancyhead[R]{\small \VAR{date}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\lstset{basicstyle=\ttfamily\small, breaklines=true, frame=single, columns=fullflexible}
\sisetup{group-separator={,}, group-minimum-digits=4}

\begin{document}

%% ═══════════════════════════════════ Title Page ═════════════════════════════
\begin{titlepage}
\centering
\vspace*{3cm}
{\Huge\bfseries Matryoshka B+ Tree:\\[0.3em]
Insert/Delete Performance Report\par}
\vspace{1.5cm}
{\Large Comparative Benchmark Results\par}
\vspace{2cm}
{\large \VAR{date}\par}
\vspace{3cm}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
CPU       & \VAR{sysinfo.cpu} \\
L1d Cache & \VAR{sysinfo.l1d} \\
L2 Cache  & \VAR{sysinfo.l2} \\
L3 Cache  & \VAR{sysinfo.l3} \\
Kernel    & \VAR{sysinfo.kernel} \\
Page Size & \VAR{sysinfo.page_size} \\
\bottomrule
\end{tabular}
\vfill
{\small Generated by \texttt{bench\_compare} --- matryoshka project\par}
\end{titlepage}

%% ═══════════════════════════════════ TOC ════════════════════════════════════
\tableofcontents
\newpage

%% ═══════════════════════════════════ Introduction ══════════════════════════
\section{Introduction}

This report evaluates the \textcolor{matryoshkacolor}{\textbf{matryoshka}}
B+ tree --- a B+ tree whose page-sized leaf nodes contain nested B+ sub-trees
of cache-line-sized (64\,B) sub-nodes, with SIMD-accelerated search at every
level --- against several tree and ordered-map libraries on \emph{insert-heavy}
and \emph{delete-heavy} workloads.  Goals:
\begin{enumerate}[nosep]
  \item Quantify the modification throughput gap across dataset sizes
        (\num{65536} to \num{16777216} keys).
  \item Identify micro-architectural bottlenecks (cache misses, TLB
        pressure, branch misprediction) that explain the differences.
\end{enumerate}
All measurements use \texttt{clock\_gettime(CLOCK\_MONOTONIC)}.  Results
are reported as \si{Mop/s} and \si{ns/op}.

%% ═══════════════════════════════════ Libraries ═════════════════════════════
\section{Library Descriptions}

\begin{table}[H]
\centering
\caption{Libraries under test.}
\label{tab:libraries}
\begin{tabular}{lll}
\toprule
\textbf{Name} & \textbf{Label} & \textbf{Description} \\
\midrule
\BLOCK{for lib in libraries}
\texttt{\VAR{lib.name}} & \VAR{lib.label} & \VAR{lib.description} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

%% ═══════════════════════════════════ Workloads ═════════════════════════════
\section{Workload Descriptions}

\begin{table}[H]
\centering
\caption{Benchmark workloads.}
\label{tab:workloads}
\begin{tabular}{lp{10cm}}
\toprule
\textbf{Workload} & \textbf{Description} \\
\midrule
\texttt{seq\_insert}
  & Insert $N$ keys in ascending order. Exercises append paths. \\
\texttt{rand\_insert}
  & Insert $N$ unique keys in random order. Stresses leaf splits. \\
\texttt{ycsb\_a}
  & 95\% insert / 5\% search. Write-dominated OLTP model. \\
\texttt{rand\_delete}
  & Bulk-load $N$ sorted keys, delete all in random order. \\
\texttt{mixed}
  & Bulk-load $N$ keys, then $N$ alternating insert/delete ops. \\
\texttt{ycsb\_b}
  & Bulk-load $N$ keys, then 50\% delete / 50\% search. \\
\texttt{search\_after\_churn}
  & Bulk-load $N$ keys, $N/2$ mixed churn (untimed), then
    \num{5000000} random predecessor searches. \\
\bottomrule
\end{tabular}
\end{table}

%% ═══════════════════════════════════ Insert-Heavy Results ══════════════════
\section{Results: Insert-Heavy Workloads}

\subsection{Sequential Insert}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_seq_insert}}
\caption{Sequential insert throughput (Mop/s).}
\label{fig:bar_seq_insert}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_seq_insert}}
\caption{Sequential insert scaling.}
\label{fig:scale_seq_insert}
\end{figure}

\subsection{Random Insert}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_rand_insert}}
\caption{Random insert throughput (Mop/s).}
\label{fig:bar_rand_insert}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_rand_insert}}
\caption{Random insert scaling.}
\label{fig:scale_rand_insert}
\end{figure}

\subsection{YCSB-A (95\% Insert / 5\% Search)}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_ycsb_a}}
\caption{YCSB-A throughput (Mop/s).}
\label{fig:bar_ycsb_a}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_ycsb_a}}
\caption{YCSB-A scaling.}
\label{fig:scale_ycsb_a}
\end{figure}

%% ═══════════════════════════════════ Delete-Heavy Results ══════════════════
\section{Results: Delete-Heavy Workloads}

\subsection{Random Delete}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_rand_delete}}
\caption{Random delete throughput (Mop/s).}
\label{fig:bar_rand_delete}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_rand_delete}}
\caption{Random delete scaling.}
\label{fig:scale_rand_delete}
\end{figure}

\subsection{Mixed Insert/Delete}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_mixed}}
\caption{Mixed insert/delete throughput (Mop/s).}
\label{fig:bar_mixed}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_mixed}}
\caption{Mixed insert/delete scaling.}
\label{fig:scale_mixed}
\end{figure}

\subsection{YCSB-B (50\% Delete / 50\% Search)}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_ycsb_b}}
\caption{YCSB-B throughput (Mop/s).}
\label{fig:bar_ycsb_b}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_ycsb_b}}
\caption{YCSB-B scaling.}
\label{fig:scale_ycsb_b}
\end{figure}

%% ═══════════════════════════════════ Search After Churn ════════════════════
\section{Results: Search After Churn}

The \texttt{search\_after\_churn} workload measures pure search
throughput on a tree that has undergone insert/delete churn,
isolating search performance from modification cost.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_search_after_churn}}
\caption{Search throughput after churn (Mop/s).}
\label{fig:bar_search_after_churn}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_search_after_churn}}
\caption{Search-after-churn scaling.}
\label{fig:scale_search_after_churn}
\end{figure}

%% ═══════════════════════════════════ Hardware Counters ═════════════════════
\section{Hardware Counter Analysis}

\BLOCK{if chart_perf}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_perf}}
\caption{Hardware counters: dTLB miss rate, LLC miss rate, IPC,
         branch misprediction rate.}
\label{fig:perf_counters}
\end{figure}

\subsection{dTLB Miss Rate}
Matryoshka's arena allocator places leaf nodes in contiguous 2\,MiB
superpage-aligned regions.  At $N = \num{\VAR{largest_n}}$, matryoshka's
dTLB miss rate is \VAR{matryoshka_dtlb_miss_rate} per \num{1000} ops,
versus \VAR{stdset_dtlb_miss_rate} for \texttt{std::set}.  Red-black
tree pointer chasing touches a new TLB entry per level; matryoshka
confines each leaf search to a single 4\,KiB page.

\subsection{LLC Miss Rate}
Matryoshka packs up to 855 keys per 4\,KiB page using a nested B+
sub-tree of cache-line sub-nodes.  \texttt{std::set} requires one
40--48\,B heap node per key.  At $N = \num{\VAR{largest_n}}$:
\VAR{matryoshka_llc_miss_rate} LLC misses/\num{1000}\,ops (matryoshka)
vs.\ \VAR{stdset_llc_miss_rate} (\texttt{std::set}).

\subsection{IPC}
SIMD search at every level (CL leaves, CL internals, outer internal
nodes) achieves IPC of \VAR{matryoshka_ipc} via pipelined
\texttt{\_mm\_cmpgt\_epi32}/\texttt{\_mm\_movemask\_ps} without
data-dependent branches.  Insert and delete paths also benefit from
SIMD navigation through the CL sub-tree to locate the target sub-node.

\subsection{Branch Misprediction}
SIMD mask arithmetic replaces conditional branches during search,
yielding near-zero misprediction.  Modification operations navigate the
CL sub-tree using the same SIMD search, with only the final CL leaf
insert/delete using scalar \texttt{memmove} on $\le 14$ keys.
\BLOCK{else}
Hardware counter data was not collected for this run.  Re-run with
\texttt{perf} support enabled (omit \texttt{-{}-no-perf}) to populate
this section with dTLB miss rates, LLC miss rates, IPC, and branch
misprediction data.
\BLOCK{endif}

%% ═══════════════════════════════════ Profiling ═════════════════════════════
\section{Profiling: Hot Functions}

\BLOCK{if profile_functions}
\begin{table}[H]
\centering
\caption{Top functions (\texttt{perf record}, rand\_insert, $N{=}\num{1048576}$).}
\label{tab:profile}
\begin{tabular}{rll}
\toprule
\textbf{\% Overhead} & \textbf{Function} & \textbf{Source} \\
\midrule
\BLOCK{for fn in profile_functions}
\num{\VAR{fn.overhead}}\% & \texttt{\VAR{fn.name}} & \texttt{\VAR{fn.source}} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

The hot functions are the page-level sub-tree operations:
\texttt{mt\_page\_insert} and \texttt{mt\_page\_delete} navigate the CL
sub-tree via SIMD search, then perform a scalar insert or delete within
a single 64\,B cache-line sub-node.  CL sub-node splits and merges
occur only on overflow or underflow.
\BLOCK{else}
Profiling data was not collected for this run.  Re-run with
\texttt{perf} support enabled (omit \texttt{-{}-no-perf}) to populate
this section.
\BLOCK{endif}

%% ═══════════════════════════════════ Cache-Miss Attribution ═══════════════
\section{Cache-Miss Attribution Analysis}

\BLOCK{if cache_miss_functions}
This section attributes cache misses to individual functions using
\texttt{perf record -e cache-misses}.  The data reveals \emph{where}
in the code the processor stalls waiting for memory, complementing
the aggregate hardware counters in Section~7.

\begin{table}[H]
\centering
\caption{Top functions by cache-miss contribution
  (matryoshka, rand\_insert, $N{=}\num{4194304}$).}
\label{tab:cache_miss_attr}
\begin{tabular}{rll}
\toprule
\textbf{\% Cache Misses} & \textbf{Function} \\
\midrule
\BLOCK{for fn in cache_miss_functions}
\num{\VAR{fn.pct}}\% & \texttt{\VAR{fn.name}} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

\subsection{Root Cause: CL Sub-Node Cache-Line Jumps}

The dominant cache-miss source is the CL sub-tree navigation within
each 4\,KiB page leaf.  When \texttt{page\_find\_leaf} descends the
CL sub-tree, each level accesses a different 64\,B slot within the
page.  These slots are not adjacent --- slot indices are assigned by
a bitmap allocator, so the root CL inode, its child CL inode, and
the target CL leaf typically reside on non-contiguous cache lines.

The hardware prefetcher cannot predict these indirect jumps (each
CL internal node stores child slot indices that must be read before
the next cache line address is known).  This creates a dependent
chain of cache misses: \emph{read child index $\rightarrow$ compute
address $\rightarrow$ miss on that address $\rightarrow$ repeat}.

At $N{=}4\text{M}$ with random inserts, the 4\,KiB pages are spread
across a $\sim$20\,MiB working set, exceeding L2 but fitting in L3.
Each page access brings the header cache line into L1, but the
CL sub-nodes deeper in the page have been evicted since the last
visit to that page.

\subsection{Optimisation: Software Prefetching}

Software prefetch hints (\texttt{\_\_builtin\_prefetch}) have been
added at three levels:

\begin{description}[style=nextline, nosep, leftmargin=1em]
  \item[\texttt{page\_find\_leaf} (CL sub-tree descent)]
    After reading the page header's \texttt{root\_slot}, the root
    CL node's cache line is prefetched before the loop begins.
    Within each loop iteration, after determining the child slot
    index from \texttt{cl\_inode\_search}, the child's cache line
    is prefetched before the next iteration accesses it.  This
    converts the dependent miss chain into overlapped prefetch +
    computation.

  \item[\texttt{find\_leaf} (outer B+ tree descent)]
    After \texttt{mt\_inode\_search} determines the child pointer,
    the first cache line of the child node is prefetched.  For
    internal-to-internal transitions, this warms the next inode's
    header and first keys.  For the last level (internal-to-leaf),
    this warms the page header so that \texttt{page\_find\_leaf}
    starts with the header in L2.

  \item[\texttt{mt\_inode\_search} (binary search in outer inodes)]
    For nodes exceeding the SIMD linear scan threshold, the binary
    search prefetches the midpoints of both candidate halves before
    each comparison.  This converts the $O(\log N)$ dependent miss
    chain of binary search into a pipelined prefetch sequence.
\end{description}

All prefetches use \texttt{locality=1} (L2 hint), appropriate for
data that will be used once in the near future but is unlikely to
be reused before eviction.

\subsection{Optimisation: Pointer Tagging for CL Root Prefetch}

Instruction-level profiling revealed that the initial software prefetching
approach had insufficient lead time: the CL root prefetch inside
\texttt{page\_find\_leaf} was issued \emph{after} loading the page header
(to read \texttt{root\_slot}), leaving only $\sim$5\,ns of computation
before the CL root access---far less than the $\sim$100\,ns needed for
an L3 miss to resolve.

To address this, leaf metadata is embedded in the low bits of child pointers
in the outer B+ tree's internal nodes.  All node pointers are 4096-byte
aligned, providing 12 guaranteed-zero low bits.  The encoding is:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Bits} & \textbf{Field} & \textbf{Range} \\
\midrule
0--5  & \texttt{root\_slot}  & 1--63 (CL sub-tree root slot index) \\
6--8  & \texttt{sub\_height} & 0--7  (CL sub-tree height) \\
9--11 & \textit{reserved}    & (available for future use, e.g.\ fullness counts) \\
\bottomrule
\end{tabular}
\end{center}

With this scheme, \texttt{find\_leaf} can extract \texttt{root\_slot} from
the tagged child pointer \emph{without} reading the page header, enabling
it to issue \emph{two} prefetches simultaneously at the last outer-tree
level:

\begin{enumerate}[nosep]
  \item The page header cache line (slot 0), as before;
  \item The CL root cache line (\texttt{slots[root\_slot $-$ 1]}),
        computed from the tag.
\end{enumerate}

Both prefetches issue at the same time, overlapping with whatever inode
search computation follows.  Tags are maintained at leaf creation
(bulk-load, split) and updated after each insert or delete that may
change \texttt{root\_slot} or \texttt{sub\_height}.  Every child pointer
read goes through an \texttt{mt\_untag()} mask operation (a single AND
instruction) to strip the low 12 bits before dereferencing.  Stale tags
on cold paths (rebalance) merely cause a wasted prefetch---never a
correctness issue.
\BLOCK{else}
Cache-miss attribution data was not collected for this run.  Re-run
with \texttt{perf} support enabled to populate this section with
per-function cache-miss breakdowns.
\BLOCK{endif}

%% ═══════════════════════════════════ Results Table ═════════════════════════
\section{Detailed Results Table}

Matryoshka rows highlighted in \colorbox{rowhl}{blue}.

\begin{longtable}{llrS[table-format=4.2]S[table-format=6.1]}
\caption{Full benchmark results.\label{tab:results}} \\
\toprule
\textbf{Library} & \textbf{Workload} & \textbf{N} &
  {\textbf{Mop/s}} & {\textbf{ns/op}} \\
\midrule
\endfirsthead
\caption[]{Full benchmark results (continued).} \\
\toprule
\textbf{Library} & \textbf{Workload} & \textbf{N} &
  {\textbf{Mop/s}} & {\textbf{ns/op}} \\
\midrule
\endhead
\midrule
\multicolumn{5}{r}{\emph{Continued on next page}} \\
\bottomrule
\endfoot
\bottomrule
\endlastfoot
\BLOCK{for r in results}
\BLOCK{if r.library == 'matryoshka' or r.library == 'matryoshka\_fence'}
\rowcolor{rowhl}
\BLOCK{endif}
\texttt{\VAR{r.library}} & \texttt{\VAR{r.workload}} &
  \num{\VAR{r.n}} & \VAR{r.mops} & \VAR{r.ns_per_op} \\
\BLOCK{endfor}
\end{longtable}

%% ═══════════════════════════════════ Analysis ══════════════════════════════
\section{Analysis and Diagnosis}

\subsection{Benchmark Access Patterns}

The seven workloads exercise distinct access patterns that interact
differently with each data structure's memory layout.  Understanding
what each workload actually measures is essential for interpreting the
results table: raw Mop/s numbers are meaningless without knowing which
bottleneck---modification overhead, cache pressure, rebalancing cost,
or in-leaf search efficiency---dominates a given workload.

\begin{description}[style=nextline, leftmargin=1em, itemsep=0.6em]
  \item[\texttt{seq\_insert} --- Sequential Append]
    Keys arrive in ascending order (1, 3, 5, \ldots).  This is the
    \emph{easiest} case for B-trees: leaves fill left-to-right with no
    splits until full, and the rightmost leaf stays hot in cache across
    consecutive inserts.  Red-black trees rebalance on each insert but
    maintain temporal locality in the allocator; ART builds monotonically
    deeper paths in byte-order with no node splitting.

    Matryoshka benefits from sequential CL sub-node filling within each
    page, but the nested sub-tree overhead---navigating two levels of CL
    internal nodes (2--3 SIMD comparisons) to reach the target CL leaf,
    then \texttt{memmove} within that 64\,B node---makes each insert more
    expensive than a simple sorted-array append in a flat B-tree leaf.
    This workload primarily tests \textbf{per-operation modification
    overhead}, not cache behaviour, since the hot working set fits in L1/L2
    regardless of structure.

  \item[\texttt{rand\_insert} --- Random Cache Pressure]
    A Fisher--Yates shuffle of $[0, N)$ scaled to odd values.  Every
    insert touches a random leaf, maximising cache pressure.  This is the
    workload most sensitive to node size and memory layout---the
    \textbf{primary benchmark for cache-conscious designs}.

    \begin{itemize}[nosep]
      \item \textbf{Wide B-tree leaves} (tlx, abseil at 256--4096\,B)
            amortise random access: one cache miss loads many keys, so a
            linear or SIMD scan within the leaf is cheap relative to the
            miss.
      \item \textbf{Pointer-chasing structures} (\texttt{std::set}) incur
            a cache miss per tree level ($\sim\!\log_2 N$ levels); at
            $N = 16\text{M}$ that is $\sim$24 dependent misses.
      \item \textbf{ART}'s fixed-depth byte-trie (4-byte key
            $\Rightarrow$ $\le 4$ levels) limits pointer-chase misses to
            at most~4, but each node may be 48--256\,B depending on type.
      \item \textbf{Matryoshka}'s 4\,KiB pages are cache-friendly, but
            the nested CL sub-tree adds 2--3 cache-line touches within
            each page.  The pointer-tagging optimisation
            (\S\ref{sec:ptrtag}) specifically targets this workload:
            prefetching the CL root while still in the outer tree removes
            one serial miss from the critical path.
    \end{itemize}

  \item[\texttt{rand\_delete} --- Rebalance Stress Test]
    Bulk-loads $N$ sorted keys (giving every structure its optimal starting
    layout), then deletes all keys in shuffled order.  This isolates
    \textbf{rebalancing cost} from insertion:

    \begin{itemize}[nosep]
      \item Red-black trees perform $O(1)$ rotations per deletion with
            small constant factors (3 pointer writes + colour flip).
      \item B-trees \texttt{memmove} within leaves and occasionally merge
            or redistribute siblings, touching 1--2 nodes.
      \item Matryoshka merges CL sub-nodes within a page (cheap---same
            cache line or adjacent lines, no system calls) and only
            performs expensive page-level merges when total occupancy drops
            below $\lfloor 855/4 \rfloor = 213$ keys.  The two-level
            underflow propagation (CL merge $\rightarrow$ page merge) is
            more complex than a flat B-tree merge but amortises well since
            most deletions only affect CL sub-nodes.
    \end{itemize}

  \item[\texttt{mixed} --- Steady-State Churn]
    Alternating insert (new key beyond current max) and delete (random
    existing key) on a pre-loaded tree of size~$N$.  The tree size
    fluctuates around $N$, creating a \textbf{realistic steady-state}
    workload.  This tests whether structures waste work on structural
    oscillation: a tree that aggressively splits a node on insert and
    immediately merges it on the next delete pays the cost of both
    operations with no net benefit.  Structures with hysteresis between
    split and merge thresholds (matryoshka uses $\text{max}/4$ for
    underflow vs.\ $\text{max}$ for split) handle this efficiently.

  \item[\texttt{ycsb\_a} (95\% insert / 5\% search) --- Write-Heavy OLTP]
    Modelled on the Yahoo!\ Cloud Serving Benchmark ``Workload~A.''
    Inserts are sequential (monotonically increasing keys), so
    \textbf{append-path efficiency} dominates the 95\% write portion.
    The 5\% predecessor searches target recently-inserted regions that
    are likely still hot in L1/L2 cache, favouring structures with good
    temporal locality in their leaf layer.  This workload reveals whether
    a structure's insert path is cheap enough to sustain high write
    throughput without being dragged down by occasional search overhead.

  \item[\texttt{ycsb\_b} (50\% delete / 50\% search) --- Shrinking Tree]
    Deletes keys from a pre-loaded tree interleaved with random
    predecessor searches.  As the tree shrinks, occupancy drops and the
    ratio of useful data to allocated memory worsens.  This tests whether
    \textbf{structural changes degrade search performance}: partially-filled
    pages waste cache capacity (fewer keys per cache miss), and ongoing
    merges may leave the tree in a suboptimal layout for search.
    Structures that reclaim space eagerly (matryoshka's CL sub-node
    merging) maintain higher effective density than those that leave
    tombstones or half-empty nodes.

  \item[\texttt{search\_after\_churn} --- Pure Search Isolation]
    The tree undergoes insert/delete churn (untimed setup phase), then
    runs \num{5000000} random predecessor searches as the timed workload.
    This \textbf{isolates search throughput} from modification cost,
    making it the purest measure of in-leaf search efficiency and memory
    layout quality.  The workload is most sensitive to:
    \begin{enumerate}[nosep]
      \item \emph{In-leaf search cost}: SIMD width (SSE2 at 4 keys vs.\
            AVX2 at 8 keys per comparison), number of CL levels traversed,
            and branch misprediction rate.
      \item \emph{Memory layout}: cache-line utilisation (how many useful
            keys per 64\,B line fetched) and whether the post-churn layout
            retains spatial locality.
      \item \emph{Tree height}: fewer outer-tree levels means fewer
            pointer-chase misses before reaching the leaf.
    \end{enumerate}
    \textbf{ART caveat:} ART lacks native predecessor search; its wrapper
    falls back to point lookup (\texttt{art\_search}), which is a
    fundamentally different and easier operation.  ART's numbers in this
    workload are not directly comparable to the other structures.
\end{description}

\paragraph{Key encoding.}
All workloads use 4-byte \texttt{int32\_t} keys encoded as odd values
($2i{+}1$), ensuring no key equals zero (used as a sentinel in CL sub-node
headers).  Keys are generated by xorshift64 with fixed seeds for
reproducibility across runs and platforms.

\subsection{Matryoshka: Nested Sub-Tree Tradeoffs}

Each insert or delete navigates the page-level CL sub-tree (2--3 SIMD
comparisons on 12--15 keys per level) to a target CL leaf, then performs
a scalar \texttt{memmove} of at most 14 keys within that 64\,B
cache-line sub-node.  The cost per modification is
$O(h_s \times b)$ where $h_s \le 2$ is the sub-tree height and
$b = 15$ is the CL leaf capacity---roughly 30--45 key touches, all
within a single 4\,KiB page.

CL sub-node splits and merges occur only when a CL leaf overflows
(15 keys) or underflows ($< 7$ keys).  Page-level splits occur only
when all 63 CL slots are exhausted ($\sim$855 keys/page).

The nested design adds a constant overhead per operation compared to
flat sorted-array B-tree leaves, where a single \texttt{memmove}
suffices.  At $N{=}\num{1048576}$: matryoshka achieves
\VAR{mat_rand_insert_mops} Mop/s on random insert, vs.\
\VAR{abseil_rand_insert_mops} (Abseil) and
\VAR{tlx_rand_insert_mops} (TLX).

However, SIMD search through the CL sub-tree is used during both
search \emph{and} the navigation phase of insert/delete.  This yields
search-after-churn throughput of \VAR{mat_search_after_churn_mops}
Mop/s at $N{=}\num{1048576}$.  The key advantage is that modifications
touch a single cache-line sub-node rather than shifting an entire
sorted leaf array.

\subsection{Detailed Comparison: std::set (Red-Black Tree)}

\texttt{std::set} uses a balanced binary search tree with one
heap-allocated node per key (40--48\,B on 64-bit systems: two child
pointers, parent pointer, colour bit, key, allocator overhead).

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{stdset_rand_insert_mops_large}
Mop/s (random insert) and \VAR{stdset_rand_delete_mops_large} Mop/s
(random delete)---the slowest of all libraries tested.
Each operation traverses $O(\log_2 N)$ levels with a pointer
dereference (and likely cache miss) at every level.  The 40--48\,B node
size means $\sim$1 useful key per cache line, so every level is a
full cache miss for large $N$.

\paragraph{Sequential insert.}
At \VAR{stdset_seq_insert_mops_large} Mop/s
($N{=}\num{\VAR{largest_n}}$), sequential insert is only modestly
better than random because the allocator provides some temporal
locality, but the red-black tree still requires $O(\log N)$ pointer
chases and rotations.

\paragraph{Search.}
Search-after-churn: \VAR{stdset_search_after_churn_mops_large} Mop/s.
Binary search through $\log_2 N \approx 24$ levels of pointer chasing
is inherently cache-unfriendly.  \texttt{std::set} has no mechanism
for SIMD-accelerated search or cache-line-aware layout.

\paragraph{Scaling.}
\texttt{std::set} shows the steepest throughput degradation from small
to large $N$ (random insert scales
\VAR{scale_stdset_rand_insert}$\times$ from $N{=}\num{\VAR{smallest_n}}$
to $N{=}\num{\VAR{largest_n}}$) because the working set of
pointer-chased nodes quickly exceeds cache capacity.

\subsection{Detailed Comparison: TLX btree\_set}

TLX implements a B+ tree with sorted-array leaves.  Leaf capacity is
typically 64--128 keys (depends on template parameters and key size).
Internal nodes use sorted arrays of separator keys with binary search.

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{tlx_rand_insert_mops_large} Mop/s
(random insert) and \VAR{tlx_rand_delete_mops_large} Mop/s (random
delete).  Each leaf insert is a binary search followed by a
\texttt{memmove} of the leaf's sorted array.  The average shift is
$B/2 \approx 32$--64 keys per insert, but the entire operation stays
within one or two cache lines for small leaves.

\paragraph{Search.}
Search-after-churn: \VAR{tlx_search_after_churn_mops_large} Mop/s.
TLX uses scalar binary search within leaves, which incurs
$\lceil\log_2 B\rceil$ comparisons with data-dependent branches.
This is slower than SIMD linear scan for the same leaf size.

\paragraph{Sequential insert.}
\VAR{tlx_seq_insert_mops_large} Mop/s.  The B+ tree append path is
efficient: new keys land at the rightmost leaf with minimal shifting,
and splits propagate only when the leaf is full.

\paragraph{Comparison to matryoshka.}
TLX's flat sorted-array leaves have a lower constant factor per
insert (one \texttt{memmove} vs.\ CL sub-tree navigation), but
matryoshka's wider pages (855 keys vs.\ $\sim$128) reduce the outer
tree height and number of leaf splits.  At large $N$, the outer-tree
traversal cost dominates, and matryoshka's SIMD-accelerated outer
internal node search closes the gap.

\subsection{Detailed Comparison: Abseil btree\_set}

Abseil's B-tree uses a similar sorted-array design to TLX but with
different node sizes and allocation strategies.  Leaf nodes hold up
to $\sim$256 keys in a single sorted array.

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{abseil_rand_insert_mops_large}
Mop/s (random insert) and \VAR{abseil_rand_delete_mops_large} Mop/s
(random delete).  The wider leaves mean fewer tree levels and splits,
but each \texttt{memmove} within a leaf shifts more keys on average
($B/2 \approx 128$).

\paragraph{Search.}
Search-after-churn: \VAR{abseil_search_after_churn_mops_large} Mop/s.
Abseil uses scalar binary search within leaves.  The wider leaves
reduce tree height (fewer pointer chases) but increase the number of
comparisons per leaf ($\lceil\log_2 256\rceil = 8$ vs.\ $\lceil\log_2
128\rceil = 7$ for TLX).

\paragraph{TLX vs.\ Abseil.}
On random insert at $N{=}\num{\VAR{largest_n}}$, TLX and Abseil are
within \VAR{btree_insert_gap_pct}\% of each other.  Abseil's wider
leaves trade cheaper outer traversal (fewer levels) for more expensive
in-leaf operations (larger \texttt{memmove}).  The two designs
converge in throughput because the cache miss cost of locating the
target leaf dominates at large $N$.

\subsection{Detailed Comparison: libart (Adaptive Radix Tree)}

ART uses a radix/trie structure with adaptive node types (Node4,
Node16, Node48, Node256) that compact sparse levels.  For 4-byte keys,
the tree has at most 4 levels regardless of $N$.

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{art_rand_insert_mops_large}
Mop/s (random insert) and \VAR{art_rand_delete_mops_large} Mop/s
(random delete).  ART's $O(k)$ complexity (key length, not tree size)
means insert cost is nearly constant across dataset sizes.  The
scaling ratio from $N{=}\num{\VAR{smallest_n}}$ to
$N{=}\num{\VAR{largest_n}}$ is \VAR{scale_art_rand_insert}$\times$
---the flattest of all structures tested.

\paragraph{Search.}
Search-after-churn: \VAR{art_search_after_churn_mops_large} Mop/s.
ART achieves the highest absolute search throughput because its point
lookups traverse $\le 4$ levels, each requiring a single indexed array
access (no comparison-based search within nodes for Node256).
\emph{However}, the benchmark uses point lookups for ART rather than
predecessor search (which ART does not natively support), so this
comparison is not apples-to-apples with the other structures.

\paragraph{Access pattern interaction.}
ART's per-byte radix decomposition means key distribution matters less
than key length.  The uniform random keys in these benchmarks create
well-distributed tries with few path-compressed nodes.  A workload
with clustered keys sharing long common prefixes would trigger more
path compression and potentially different performance
characteristics.

\paragraph{Memory overhead.}
ART's adaptive node types (4, 16, 48, or 256 children) trade memory
for access speed.  At high occupancy, most internal nodes are Node48
or Node256, using 256--2048\,B per node regardless of actual
fan-out---significantly more memory per key than B-tree or matryoshka
designs.

\subsection{Hardware Counter Comparison}

\BLOCK{if hw_rows}
\begin{table}[H]
\centering
\caption{Hardware counters across all libraries (rand\_insert, $N{=}\num{\VAR{largest_n}}$).}
\label{tab:hw_compare}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Library} & \textbf{Cache Miss} & \textbf{L1d Miss}
  & \textbf{LLC Miss} & \textbf{dTLB Miss} & \textbf{Branch Miss}
  & \textbf{IPC} \\
  & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)}
  & \textbf{(/1K ops)} & \textbf{(\%)} & \\
\midrule
\BLOCK{for row in hw_rows}
\BLOCK{if row.name == 'matryoshka' or row.name == 'matryoshka\_fence'}
\rowcolor{rowhl}
\BLOCK{endif}
\texttt{\VAR{row.name}} & \VAR{row.cache_miss_pct}
  & \VAR{row.l1d_miss_pct} & \VAR{row.llc_miss_pct}
  & \VAR{row.dtlb_miss_per_1k} & \VAR{row.branch_miss_pct}
  & \VAR{row.ipc} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

The hardware counters reveal distinct micro-architectural profiles:

\paragraph{dTLB pressure.}
Matryoshka's arena allocator (2\,MiB hugepage-backed regions) yields
the lowest dTLB miss rate (\VAR{matryoshka_dtlb_miss_rate}/\num{1000}
ops).  \texttt{std::set} has the highest
(\VAR{stdset_dtlb_miss_rate}/\num{1000}) because each pointer chase
to a heap-allocated node potentially touches a new 4\,KiB page.
The B-tree libraries (TLX, Abseil) fall between: their wider nodes
reduce the number of distinct pages accessed per operation, but
they use standard \texttt{malloc} without hugepage awareness.
ART's adaptive nodes are heap-allocated but fewer in number than
red-black tree nodes, yielding moderate dTLB pressure.

\paragraph{LLC misses.}
At $N{=}\num{\VAR{largest_n}}$ (random insert), all structures exceed
LLC capacity.  Matryoshka's LLC miss rate is
\VAR{matryoshka_llc_miss_rate}/\num{1000} ops vs.\
\VAR{stdset_llc_miss_rate} for \texttt{std::set}.  Matryoshka
confines each leaf operation to a single 4\,KiB page (or a few
adjacent cache lines for CL sub-node navigation), while red-black
tree operations touch $\log_2 N$ widely-scattered nodes.  The B-tree
libraries achieve comparable LLC rates because their wide leaves
also localise work.

\paragraph{IPC.}
Matryoshka achieves IPC of \VAR{matryoshka_ipc} via SIMD search
(\texttt{\_mm\_cmpgt\_epi32}/\texttt{\_mm\_movemask\_ps}) at every
level, avoiding data-dependent branches.  \texttt{std::set}'s
pointer-chasing and branch-heavy traversal yields the lowest IPC.
ART's indexed array lookups (no comparisons for Node256) can achieve
high IPC on cache-hot paths.

\paragraph{Branch misprediction.}
SIMD mask arithmetic in matryoshka replaces conditional branches,
yielding low branch misprediction rates.  \texttt{std::set} and the
scalar-binary-search B-trees (TLX, Abseil) have higher
misprediction rates because each comparison is a data-dependent
branch.  ART's indexed-lookup approach avoids comparison branches
within nodes but has conditional branches for node type dispatch.
\BLOCK{else}
Hardware counter data was not collected for this run.  Re-run with
\texttt{perf} support enabled (omit \texttt{-{}-no-perf}) to populate
this section.
\BLOCK{endif}

\subsection{Access Pattern Interactions with Data Structure Layout}

The interaction between access pattern and memory layout explains
much of the performance variation:

\paragraph{Sequential vs.\ random insert.}
Sequential insert favours structures with efficient append paths.
All B-tree variants (matryoshka, TLX, Abseil) benefit because new
keys land at the rightmost leaf.  \texttt{std::set} benefits less
because red-black rebalancing is oblivious to key order.  The
throughput ratio (seq/rand) at $N{=}\num{\VAR{largest_n}}$ reveals
how much each structure benefits from locality:
matryoshka \VAR{ratio_mat_stdset_seq_insert}$\times$ vs.\
std::set on sequential, \VAR{ratio_mat_stdset_rand_insert}$\times$
on random.

\paragraph{Delete after bulk-load vs.\ interleaved.}
The \texttt{rand\_delete} workload starts from a bulk-loaded
(optimally packed) tree, giving every structure its best starting
point.  The \texttt{mixed} workload, by contrast, operates on a
tree that is continuously modified, creating internal fragmentation.
Structures that maintain good occupancy under churn (B-trees with
merge/redistribute) degrade less between these workloads than
structures with per-node allocation (\texttt{std::set}).

\paragraph{The 4\,KiB page boundary.}
Matryoshka's 4\,KiB page leaves are sized to match the OS page size,
ensuring that navigating the CL sub-tree within a leaf never crosses a
page boundary.  TLX and Abseil leaves are smaller ($<$1\,KiB), so
multiple leaves may share a page---good for spatial locality of
adjacent leaves, but each leaf may straddle two cache lines for the
\texttt{memmove} operation.  \texttt{std::set} nodes are scattered
across the heap with no page-alignment guarantees.

\paragraph{SIMD and branch prediction.}
Matryoshka's SIMD search produces a bit mask rather than a conditional
branch, making it prediction-friendly.  The sorted-array B-trees
(TLX, Abseil) use scalar binary search with $O(\log B)$
data-dependent branches per leaf, which the branch predictor
struggles with for uniform random keys (50/50 taken probability
at each comparison).

\subsection{Proposed Additional Access Patterns}

Several workloads not currently benchmarked would reveal different
performance relationships:

\begin{description}[style=nextline, nosep, leftmargin=1em]
  \item[Zipfian (skewed) insert]
    A Zipfian distribution concentrates inserts on a small number of
    ``hot'' leaves.  B-tree variants would benefit from cache-hot leaves;
    \texttt{std::set} would benefit from a cache-hot path of recently
    accessed nodes.  Matryoshka's per-page CL sub-tree might show
    more frequent CL splits under concentrated load.

  \item[Range scan after insert]
    Iterate over a range of $k$ keys (e.g.\ $k = 1000$) after building
    the tree.  Matryoshka's linked leaf pages and dense packing should
    excel; \texttt{std::set}'s in-order traversal via parent pointers
    would lag.  This would highlight the spatial locality advantage of
    contiguous leaf storage.

  \item[Interleaved point lookup and insert]
    A read-modify-write pattern (``contains then insert if absent'')
    would test whether search and insert share cache-hot state.
    Matryoshka's search and insert paths share the same CL sub-tree
    navigation code, so a just-searched path remains cache-hot for
    the subsequent insert.

  \item[Large-key workload]
    Keys longer than 4 bytes (e.g.\ 16- or 32-byte strings) would
    stress ART's strength (key-length-dependent, not N-dependent
    traversal) while increasing matryoshka's CL sub-node overhead
    (fewer keys per 64\,B cache line).  B-tree \texttt{memmove}
    cost would grow linearly with key size.

  \item[Delete-heavy with searches (YCSB-D)]
    A workload where the tree shrinks from $N$ to near-empty while
    servicing read queries.  This would stress merge and redistribute
    paths, and test whether search throughput degrades as the tree
    becomes sparsely populated.  Matryoshka's CL sub-node merge and
    page-level redistribute are designed for graceful degradation, but
    extreme sparsity (few keys spread across many pages) could hurt
    cache utilisation.

  \item[Bulk-load comparison]
    Timing the bulk-load operation itself (currently untimed) would
    highlight structural differences: matryoshka distributes keys
    across CL sub-nodes within pages in a single bottom-up pass;
    TLX and Abseil use repeated insertion; \texttt{std::set} has no
    bulk-load optimisation.
\end{description}

\subsection{Overall Assessment}

The matryoshka nesting design achieves competitive insert and delete
throughput while preserving SIMD-accelerated search at every level of
the hierarchy.  Each modification touches a single cache-line sub-node
rather than rebuilding an entire sorted leaf array.  At the largest
dataset size ($N{=}\num{\VAR{largest_n}}$), matryoshka's throughput is
within \VAR{insert_slowdown_factor}$\times$ of the best B-tree
competitor on insert-heavy workloads and
\VAR{delete_slowdown_factor}$\times$ on delete-heavy workloads.

The primary cost of the nesting design is a constant-factor overhead
per modification (CL sub-tree navigation), which the flat sorted-array
B-trees avoid.  This overhead is most visible at small $N$ where the
outer tree is shallow and the per-operation cost is dominated by
in-leaf work.  At large $N$, where the outer tree traversal and cache
misses dominate, the nesting overhead is amortised and matryoshka's
SIMD search and dense page layout become decisive advantages.

%% ═══════════════════════════════════ Recommendations ═══════════════════════
\section{Improvements Since Initial Report}

\subsection{Superpage-Level Nesting (Implemented)}

The nesting now extends to three levels: CL sub-nodes (64\,B) within
4\,KiB pages within 2\,MiB superpages.  Each superpage contains a
B+ tree of page-level sub-nodes, with page-level internal nodes
(681 separator keys, 682 children per 4\,KiB page) routing searches
to up to 510 page leaves.  Maximum capacity per superpage:
$510 \times 855 \approx 436\text{K}$ keys.  This confines most
operations to a single TLB entry and reduces outer-tree height.
Enable via \texttt{mt\_hierarchy\_init\_superpage}.

\subsection{Wider SIMD: AVX2 and AVX-512 (Implemented)}

Compile-time SIMD width selection via \texttt{-DMT\_SIMD=avx2} or
\texttt{-DMT\_SIMD=avx512}.  AVX2 (256-bit) processes 8 keys per
comparison in CL leaf predecessor search, CL internal search, and
outer internal node search.  AVX-512 (512-bit) processes 16 keys
per comparison using masked operations.  Unaligned loads handle the
4-byte header offset within CL sub-nodes.  SSE2 (128-bit, 4 keys)
remains the baseline fallback.

\subsection{Batch Insert and Delete API (Implemented)}

\texttt{matryoshka\_insert\_batch(tree, keys, n)} and
\texttt{matryoshka\_delete\_batch(tree, keys, n)} sort incoming keys,
group them by target leaf, and amortise outer-tree traversal across
each group.  On page-full or underflow, the path is re-navigated for
remaining keys.  Both functions work with page leaves and superpages.

\subsection{CL Sub-Tree Cache-Miss Optimisation: Fence Keys vs.\ Eytzinger (Implemented)}
\label{sec:fence_eytz}

Instruction-level profiling (\texttt{perf record -e cache-misses} with
\texttt{addr2line}) of the baseline matryoshka on \texttt{rand\_insert}
at $N{=}16\text{M}$ identified three serial cache-miss hotspots within
the CL sub-tree traversal:

\begin{center}
\begin{tabular}{rlp{8cm}}
\toprule
\textbf{\% Miss} & \textbf{Source} & \textbf{Description} \\
\midrule
59.8\% & \texttt{leaf.c:323} & \texttt{page->header.sub\_height} --- first access to the page header cache line \\
38.0\% & \texttt{leaf.c:199} & \texttt{cl->nkeys} in \texttt{cl\_inode\_search} --- loading the child CL internal node \\
76.3\%\rlap{${}^*$} & \texttt{leaf.c:451} & \texttt{cl->nkeys < MT\_CL\_KEY\_CAP} in \texttt{mt\_page\_insert} --- loading the target CL leaf \\
\bottomrule
\multicolumn{3}{l}{\small ${}^*$Percentage of \texttt{mt\_page\_insert}'s cache misses, not total.}
\end{tabular}
\end{center}

These form a serial dependency chain: header $\rightarrow$ CL root
internal $\rightarrow$ child CL node.  Each load depends on data
from the previous load, so no out-of-order execution or hardware
prefetching can overlap them.  The pointer-tagging optimisation
(\S\ref{sec:ptrtag}) already addresses hotspot~1 by prefetching the
CL root from tagged pointers while still in the outer tree, but
hotspots~2 and~3 remain.  Two strategies were implemented and
benchmarked head-to-head:

\subsubsection{Strategy A: Fence Keys}

Store the CL root internal's separator keys and child slot indices
in the 32 spare bytes of the page header (previously \texttt{\_reserved}).
Since the page header is always loaded first (hotspot~1 is
unavoidable), the fence data comes ``for free''---the CL root
internal can be skipped entirely for height-1 sub-trees with
$\le 6$ separators.

\paragraph{Header layout.}
\begin{lstlisting}[language=C, xleftmargin=2em]
/* Replaces _reserved[32] in mt_page_header_t */
int32_t  fence_keys[6];    /* 24 bytes */
uint8_t  fence_slots[7];   /* 7 bytes  */
uint8_t  nfence;           /* 1 byte   */
                            /* 32 bytes total */
\end{lstlisting}

\paragraph{When fence keys apply.}
\begin{itemize}[nosep]
  \item Height~1, $\le 7$ children: fence keys fully resolve the
        CL leaf $\rightarrow$ \textbf{skip CL internal entirely}
  \item Height~2, root has $\le 6$ children: fence keys skip the
        root internal $\rightarrow$ saves one level
  \item Height~1, 8--13 children: \texttt{nfence=0}, transparent
        fallback to normal path
\end{itemize}

\paragraph{Maintenance.}
A \texttt{refresh\_fence\_keys()} helper copies
$\min(\text{nkeys}, 6)$ keys and $\min(\text{nkeys}+1, 7)$ child
slots from the CL root internal into the header.  It is called after
bulk load, CL root split (in \texttt{mt\_page\_insert}), and CL root
collapse (in \texttt{mt\_page\_delete}).

\paragraph{Page capacity.}
Unchanged (855 keys/page).  Outer tree structure unchanged.
Enable via \texttt{mt\_hierarchy\_init\_fence}.

\subsubsection{Strategy B: Eytzinger Dense BFS Layout}

Fix the CL sub-tree to height $\le 1$ with a dense BFS layout.
The root always occupies slot~1; children occupy slots
$2, 3, \ldots, N{+}1$.  Since child positions are arithmetic (not
stored in the CL internal), all children can be prefetched
simultaneously while the root cache line is still loading---breaking
the dependency chain between hotspots~1 and~2.

\paragraph{CL internal type.}
A new 64-byte Eytzinger internal stores 15 separator keys with no
\texttt{children[]} array (positions are implicit):
\begin{lstlisting}[language=C, xleftmargin=2em]
typedef struct mt_cl_inode_eytz {
    uint8_t  type;       /* MT_CL_INTERNAL  */
    uint8_t  nkeys;      /* 0-15            */
    uint8_t  nchildren;  /* 1-16            */
    uint8_t  _pad;
    int32_t  keys[15];   /* 60 bytes        */
} mt_cl_inode_eytz_t;   /* 64 bytes total  */
\end{lstlisting}

\paragraph{Key trade-off.}
With 16 children $\times$ 15 keys $=$ 240 keys max per page, pages
split at 240 keys instead of 855.  This means $\sim$3.5$\times$ more
pages in the outer tree, but each page operation is faster due to
the eliminated serial miss.  When a CL leaf splits within an
Eytzinger page, the dense BFS invariant is restored by extracting
all keys and rebuilding the layout---amortised to $\sim$16 key
copies per insert.

Enable via \texttt{mt\_hierarchy\_init\_eytzinger}.

\subsubsection{Benchmark Results}

\begin{table}[H]
\centering
\caption{Fence keys vs.\ Eytzinger vs.\ baseline: \texttt{rand\_insert} (ns/op, lower is better).}
\label{tab:fence_eytz_insert}
\begin{tabular}{rS[table-format=3.0]S[table-format=3.0]rS[table-format=4.0]r}
\toprule
\textbf{N} & {\textbf{Baseline}} & {\textbf{Fence}} & \textbf{$\Delta$}
  & {\textbf{Eytzinger}} & \textbf{$\Delta$} \\
\midrule
\num{65536}    & 307 & 245 & $-$20\% & 791  & $+$158\% \\
\num{262144}   & 352 & 276 & $-$21\% & 819  & $+$133\% \\
\num{1048576}  & 406 & 388 & $-$4\%  & 803  & $+$98\%  \\
\num{4194304}  & 652 & 510 & $-$22\% & 1051 & $+$61\%  \\
\num{16777216} & 719 & 706 & $-$2\%  & 1123 & $+$56\%  \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Fence keys vs.\ Eytzinger vs.\ baseline: \texttt{search\_after\_churn} (ns/op).}
\label{tab:fence_eytz_search}
\begin{tabular}{rS[table-format=3.0]S[table-format=3.0]rS[table-format=3.0]r}
\toprule
\textbf{N} & {\textbf{Baseline}} & {\textbf{Fence}} & \textbf{$\Delta$}
  & {\textbf{Eytzinger}} & \textbf{$\Delta$} \\
\midrule
\num{65536}    & 232 & 193 & $-$17\% & 228 & $-$2\%  \\
\num{16777216} & 785 & 668 & $-$15\% & 771 & $-$2\%  \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Hardware Counter Analysis}

\texttt{perf stat} on \texttt{rand\_insert} at $N{=}16\text{M}$ (P-core):

\begin{table}[H]
\centering
\caption{Hardware counters: fence keys vs.\ Eytzinger vs.\ baseline.}
\label{tab:fence_eytz_hw}
\begin{tabular}{lS[table-format=3.0]S[table-format=4.0]S[table-format=2.1]S[table-format=1.2]}
\toprule
\textbf{Variant} & {\textbf{Cache Miss (M)}} & {\textbf{L1d Miss (M)}}
  & {\textbf{Instr (B)}} & {\textbf{IPC}} \\
\midrule
Baseline   & 393 & 145 & 12.6 & 0.65 \\
Fence      & 384 & 130 & 12.7 & 0.69 \\
Eytzinger  & 504 & 172 & 25.3 & 0.85 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Fence keys} reduced L1d misses by 10.3\% and total cycles
by 4.5\%, with IPC improving from 0.65 to 0.69.  The instruction count
is essentially unchanged (+1\%), confirming that the fence key fast
path adds negligible overhead---it merely resolves the CL leaf from
data already in the header cache line.

\paragraph{Eytzinger} achieved dramatically better IPC (0.85 vs.\ 0.65)
and a lower cache miss \emph{rate} (43.8\% vs.\ 66.6\%), confirming
that the mass-prefetch of all 16 children is working as intended:
the CPU executes useful computation while prefetches resolve in
parallel.  However, the absolute miss count is 28\% higher (504M
vs.\ 393M) and the instruction count doubled (25.3B vs.\ 12.6B)
due to the taller outer tree ($3.5\times$ more pages) and the
$O(240)$ extract-rebuild on every CL leaf split.

\subsubsection{Cache-Miss Profile Shift with Fence Keys}

\texttt{perf record -e cache-misses} on \texttt{rand\_insert} at
$N{=}16\text{M}$:

\begin{table}[H]
\centering
\caption{Cache-miss attribution: baseline vs.\ fence keys.}
\label{tab:fence_cachemiss}
\begin{tabular}{lrrl}
\toprule
\textbf{Function} & \textbf{Baseline} & \textbf{Fence} & \textbf{Change} \\
\midrule
\texttt{page\_find\_leaf}  & 51.6\% & 48.2\% & $-$3.4\,pp \\
\quad \textit{of which} \texttt{cl\_inode\_search} & 31.7\% & 22.6\% & $-$9.1\,pp \\
\texttt{mt\_page\_insert}  & 25.3\% & 28.8\% & $+$3.5\,pp \\
\texttt{mt\_inode\_search} & 14.2\% & 14.2\% & $\sim$0 \\
\texttt{matryoshka\_insert} & 3.0\%  & 3.1\%  & $\sim$0 \\
\bottomrule
\end{tabular}
\end{table}

The fence keys reduced \texttt{cl\_inode\_search} cache misses by 9.1
percentage points---exactly the CL root internal loads being skipped
for height-1 sub-trees with $\le 6$ separators.  The residual 22.6\%
comes from: (1)~height-2 pages where fence keys skip the root but a
level-1 internal still requires loading; and (2)~height-1 pages with
7--12 separators that exceed the 6-key fence capacity.

Instruction-level attribution of the remaining hotspots with fence
keys:

\begin{description}[style=nextline, nosep, leftmargin=1em]
  \item[\texttt{page\_find\_leaf} at \texttt{leaf.c:199} (35.6\%)]
    \texttt{int n = cl->nkeys} in \texttt{cl\_inode\_search}---loading
    the child CL internal after fence keys resolved the root.  This
    fires on height-2 pages and height-1 pages with $> 6$ separators.

  \item[\texttt{mt\_page\_insert} at \texttt{leaf.c:62} (17.7\% overall)]
    \texttt{int lo = 0, hi = cl->nkeys} in
    \texttt{cl\_leaf\_lower\_bound}---loading the target CL leaf to
    find the insertion point.  This is the third miss in the serial
    chain: header $\rightarrow$ (fence skip root) $\rightarrow$ child
    CL leaf.  It is now the \textbf{dominant remaining bottleneck}.

  \item[\texttt{mt\_inode\_search} at \texttt{inode.c:91} (13.6\%)]
    Binary search loop within outer tree internal nodes---standard
    outer-tree traversal cost, unaffected by CL-level optimisations.
\end{description}

\subsubsection{Conclusions}

\begin{enumerate}[nosep]
  \item \textbf{Fence keys are the clear winner.}  17--22\% faster
        on insert and search at sizes fitting L3 cache (up to
        $N{=}4\text{M}$), with 2--15\% improvement persisting at
        $N{=}16\text{M}$.  Zero impact on page capacity or outer
        tree structure.

  \item \textbf{Eytzinger is a negative trade-off.}  Despite
        achieving better IPC (0.85 vs.\ 0.65) and a lower cache
        miss rate, the $3.5\times$ page count increase
        (240 vs.\ 855 keys/page) and $O(240)$ extract-rebuild on
        every CL leaf split make it consistently slower---56--158\%
        slower on insert across all sizes.

  \item \textbf{The dominant remaining bottleneck} is loading the
        target CL leaf (\texttt{cl\_leaf\_lower\_bound} at 17.7\%
        of total cache misses).  This is the irreducible third step
        in the serial chain that neither strategy can eliminate: the
        CL leaf's address is only known after searching the CL
        internal one level above.

  \item \textbf{Fence keys hurt delete} by $\sim$15\% due to the
        \texttt{refresh\_fence\_keys()} overhead after structural
        changes (merge, redistribute, root collapse).  Future work
        could defer fence refresh to the next search, amortising the
        cost across multiple modifications.
\end{enumerate}

\subsubsection{Follow-Up: Mass Prefetch of Fence Children}
\label{sec:mass_prefetch}

The remaining bottleneck after fence keys is loading the target CL leaf
(\texttt{cl\_leaf\_lower\_bound} at 17.7\% of total cache misses).
The fence key fast path already knew all child slot positions (from
\texttt{fence\_slots[]} in the header), but issued a prefetch only
\emph{after} the fence search resolved the target child---leaving
insufficient latency hiding between the prefetch and the first access.

The fix: prefetch \emph{all} fence children \emph{before} the fence
search.  At most 7 prefetches ($7 \times 64\text{\,B} = 448$\,B),
all within the same 4\,KiB page (single TLB entry).  Spurious
prefetches cost only L1/L2 fill bandwidth.  The fence search
($\le 6$ comparisons) provides the latency window for the target
child's cache line to arrive.

\begin{lstlisting}[language=C, xleftmargin=2em]
/* Prefetch ALL fence children before searching. */
int nf = page->header.nfence;
for (int c = 0; c <= nf; c++)
    __builtin_prefetch(get_slot_c(page,
                       page->header.fence_slots[c]), 0, 1);
int ci = fence_search(page->header.fence_keys, nf, key);
\end{lstlisting}

\paragraph{Throughput results (ns/op, lower is better).}
\begin{table}[H]
\centering
\caption{Mass prefetch vs.\ fence (no mass prefetch) vs.\ baseline.}
\label{tab:mass_prefetch}
\begin{tabular}{llS[table-format=3.1]S[table-format=3.1]S[table-format=3.1]r}
\toprule
\textbf{Workload} & \textbf{N}
  & {\textbf{Baseline}} & {\textbf{Fence}} & {\textbf{Fence+MP}} & \textbf{$\Delta$ vs Base} \\
\midrule
rand\_insert        & 4M  & 768.4 & 510   & 577.2 & $-$24.9\% \\
rand\_insert        & 16M & 801.5 & 706   & 784.9 & $-$2.1\%  \\
search\_after\_churn & 16M & 769.5 & 668   & 707.5 & $-$8.1\%  \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Hardware counters (P-core, \texttt{rand\_insert}, $N{=}16\text{M}$).}
\begin{table}[H]
\centering
\caption{Hardware counters: mass prefetch vs.\ baseline.}
\label{tab:mass_prefetch_hw}
\begin{tabular}{lS[table-format=3.0]S[table-format=3.0]S[table-format=2.1]S[table-format=1.2]}
\toprule
\textbf{Variant} & {\textbf{Cache Miss (M)}} & {\textbf{L1d Miss (M)}}
  & {\textbf{Instr (B)}} & {\textbf{IPC}} \\
\midrule
Baseline     & 364 & 145 & 12.5 & 0.66 \\
Fence+MP     & 467 & 131 & 12.7 & 0.71 \\
\bottomrule
\end{tabular}
\end{table}

The mass prefetch reduced L1d misses by 9.7\% (145M $\rightarrow$ 131M)
and improved IPC from 0.66 to 0.71.  Total reported ``cache misses'' rose
(364M $\rightarrow$ 467M) because hardware counters include the
speculative prefetches that hit L2/L3 but were not consumed---the
\emph{useful} miss rate (misses that stall the pipeline) decreased,
as evidenced by the 4\% wall-clock improvement at $N{=}16\text{M}$
and 25\% at $N{=}4\text{M}$.

\paragraph{Cache-miss attribution with mass prefetch.}
\begin{table}[H]
\centering
\caption{Cache-miss profile: fence+mass prefetch vs.\ baseline.}
\label{tab:mass_prefetch_cachemiss}
\begin{tabular}{lrrl}
\toprule
\textbf{Function} & \textbf{Baseline} & \textbf{Fence+MP} & \textbf{Change} \\
\midrule
\texttt{page\_find\_leaf}   & 48.2\% & 32.0\% & $-$16.2\,pp \\
\texttt{mt\_page\_insert}   & 28.8\% & 22.2\% & $-$6.6\,pp \\
\texttt{mt\_inode\_search}  & 13.6\% & 2.8\%  & $-$10.8\,pp \\
\texttt{make\_shuffled\_keys} & ---  & 34.5\% & (setup noise) \\
\bottomrule
\end{tabular}
\end{table}

Instruction-level annotation confirms the prefetch loop itself incurs
0\% cache misses (the prefetch instructions never stall), and
\texttt{page\_find\_leaf}'s remaining 32\% is entirely the unavoidable
first touch of the page header (\texttt{sub\_height} load at function
entry).  The 88\% concentration of \texttt{mt\_page\_insert}'s misses
at \texttt{cl->nkeys} (offset~1 of the CL leaf) confirms that the
serial chain has been compressed to its theoretical minimum: header
$\rightarrow$ CL leaf, with the CL root internal eliminated by fence
keys and the CL leaf latency partially hidden by mass prefetch.

\subsubsection{Cross-Library Hardware Counter Comparison}
\label{sec:cross_lib_hw}

\begin{table}[H]
\centering
\caption{Hardware counters across all libraries (\texttt{rand\_insert},
  $N{=}16\text{M}$, P-core).}
\label{tab:cross_lib_hw}
\begin{tabular}{lS[table-format=2.1]S[table-format=3.0]S[table-format=3.0]S[table-format=3.0]S[table-format=1.2]r}
\toprule
\textbf{Library} & {\textbf{Instr (B)}} & {\textbf{L1d Miss (M)}}
  & {\textbf{Cache Miss (M)}} & {\textbf{LLC Miss (M)}}
  & {\textbf{IPC}} & \textbf{ns/op} \\
\midrule
\rowcolor{rowhl}
matryoshka (fence+MP) & 12.7 & 131 & 467 & 17.9 & 0.71 & 727 \\
matryoshka (baseline) & 12.5 & 145 & 364 & 15.8 & 0.66 & 757 \\
Abseil btree          & 13.4 & 285 & 436 & 93.1 & 0.76 & 737 \\
TLX btree             & 11.7 & 212 & 267 & 46.0 & 0.60 & 857 \\
libart (ART)          & 13.9 & 308 & 495 & 137.7 & 0.91 & 384 \\
std::set (RB tree)    & 11.1 & 652 & 710 & 245.5 & 0.20 & 2741 \\
\bottomrule
\end{tabular}
\end{table}

Key observations:

\begin{description}[style=nextline, nosep, leftmargin=1em]
  \item[Matryoshka has the fewest L1d misses (131M) and LLC misses (17.9M).]
    The 4\,KiB page design confines each operation to a single page, and
    the arena allocator's hugepage-backed regions keep the outer tree
    compact.  Abseil has $2.2\times$ the L1d misses and $5.2\times$ the
    LLC misses despite similar throughput---it compensates with wider
    SIMD-free leaves that amortise each miss over more keys.

  \item[libart achieves the highest IPC (0.91) and throughput (384\,ns/op).]
    ART's fixed-depth radix trie ($\le 4$ levels for 4-byte keys) limits
    the serial dependency chain to $\le 4$ pointer chases.  Its high cache
    miss count (495M) and LLC miss count (137.7M) are tolerated because
    each miss resolves an entire trie level, and the short chain means the
    CPU spends less time stalled per operation.  ART also has the lowest
    branch misprediction rate (1.2\%), since node dispatch uses indexed
    array lookups rather than comparison branches.

  \item[std::set is memory-bound (IPC~0.20).]
    Each of the $\sim$24 pointer chases per operation is a full cache miss,
    and the CPU stalls waiting for each one before computing the next
    address.  The 652M L1d misses and 245.5M LLC misses reflect one
    miss per tree level per operation.  Despite having the lowest
    instruction count (11.1B), the serial dependency chain dominates.

  \item[TLX has the fewest total cache misses (267M) but lowest IPC (0.60).]
    TLX's scalar binary search within sorted-array leaves uses
    data-dependent branches that the CPU struggles to predict (10\%
    branch miss rate), causing pipeline flushes on every misprediction.
    The low cache miss count reflects good spatial locality of the
    moderate-sized leaves, but the branch mispredictions prevent the
    CPU from filling the pipeline efficiently.
\end{description}

\subsection{Bottleneck Analysis at $N{=}16\text{M}$ (Fence + Mass Prefetch)}
\label{sec:bottleneck_16m}

At $N{=}16\text{M}$ with \texttt{rand\_insert}, the fence + mass
prefetch variant achieves 776\,ns/op (1.29\,Mop/s) with the following
per-operation budget:

\begin{table}[H]
\centering
\caption{Per-operation budget (\texttt{rand\_insert}, $N{=}16\text{M}$,
  P-core at $\sim$1.4\,GHz effective).}
\label{tab:op_budget}
\begin{tabular}{lS[table-format=3.0]S[table-format=2.1]r}
\toprule
\textbf{Function} & {\textbf{Cycles/op}} & {\textbf{\% cycles}}
  & \textbf{Role} \\
\midrule
\texttt{page\_find\_leaf}   & 412 & 37.3 & Fence search + mass prefetch + CL sub-tree descent \\
\texttt{mt\_page\_insert}   & 280 & 25.3 & CL leaf binary search + insert + split propagation \\
\texttt{mt\_inode\_search}  & 240 & 21.7 & Outer tree binary search (339-key inodes) \\
\texttt{matryoshka\_insert} & 75  & 6.8  & \texttt{find\_leaf} dispatch + pointer retag \\
Other                        & 99  & 8.9  & \texttt{memmove}, setup, shuffled-key generation \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Cache-miss attribution (\texttt{rand\_insert}, $N{=}16\text{M}$, P-core).}
\label{tab:cachemiss_16m}
\begin{tabular}{lrl}
\toprule
\textbf{Function} & \textbf{\% misses} & \textbf{Dominant instruction} \\
\midrule
\texttt{page\_find\_leaf}  & 44.1\% & \texttt{sub\_height} load --- page header first touch \\
\texttt{mt\_page\_insert}  & 34.9\% & \texttt{cl->nkeys} load --- CL leaf first touch \\
\texttt{mt\_inode\_search} & 13.5\% & Binary search \texttt{cmp} + prefetch loads in loop \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Bottleneck 1: Page Header Cold Miss (44\% of cache misses)}

The first touch of each 4\,KiB leaf page (\texttt{sub\_height} at
byte offset 5) accounts for 44\% of all cache misses.  At $N{=}16\text{M}$,
the working set is $\sim$78K pages $\times$ 4\,KiB $= \sim$312\,MiB of
leaf data --- far exceeding the 24\,MB L3 cache.  Only $\sim$6K of the
78K pages fit in L3 at any time, yielding a $\sim$92\% miss rate on
random page access.

The pointer-tag prefetch in \texttt{find\_leaf} fires the page header
prefetch at the last outer-tree level, but only $\sim$30 cycles of
inode search computation separate the prefetch from the first access
in \texttt{page\_find\_leaf} --- insufficient to hide an LLC miss
($\sim$40--70 cycles) or DRAM miss ($\sim$200 cycles).

\subsubsection{Bottleneck 2: CL Leaf Cold Miss (35\% of cache misses)}

After fence keys resolve the target CL leaf slot, the mass prefetch
fires all fence children before the fence search.  The fence search
provides $\sim$12--18 cycles of latency window (6 comparisons).  At
$N{=}16\text{M}$, the CL leaf's cache line has been evicted since the
last visit to this page, and $\sim$18 cycles is insufficient to hide
the LLC/DRAM latency.  The prefetch converts a \emph{stall} into a
\emph{partial overlap}, but the residual miss remains the dominant
bottleneck within \texttt{mt\_page\_insert} (74.4\% of its cache misses
at the \texttt{cl->nkeys} load).

\subsubsection{Bottleneck 3: Outer Inode Binary Search (13.5\% of cache misses, 21.7\% of cycles)}

\texttt{mt\_inode\_search} binary-searches 339-key outer internal nodes
(4\,KiB each, spanning 64 cache lines).  The binary search touches
$\lceil\log_2 339\rceil \approx 9$ cache lines per search, with the
prefetch-both-halves technique overlapping adjacent iterations.  The
prefetch instructions themselves show 0\% cache misses (they never
stall), but the \emph{data loads} after the prefetch still miss when
the prefetch hasn't completed in time.  The cache misses are spread
across the loop body: \texttt{cmp} instruction at 5.3\%, \texttt{movslq}
at 12.9\%, and prefetch targets at 7.6\%.

At $N{=}16\text{M}$ the outer tree has height~1 (root inode with
$\sim$230 children).  The root inode's 4\,KiB page spans 64 cache lines;
repeated accesses keep the hot lines in L2, but the binary search's
access pattern touches $\sim$9 lines per query across different
regions of the node.  Branch misprediction contributes 9.3\% of all
branches missed ($\sim$11 mispredicts per insert), with $\sim$4 coming
from the outer inode binary search (data-dependent \texttt{cmp/jle}
at $\sim$50\% taken probability).

\subsubsection{Hardware Counters at $N{=}16\text{M}$}

\begin{table}[H]
\centering
\caption{Hardware counters: fence + mass prefetch
  (\texttt{rand\_insert}, $N{=}16\text{M}$, P-core).}
\label{tab:hw_16m}
\begin{tabular}{lr}
\toprule
\textbf{Counter} & \textbf{Value} \\
\midrule
Cycles           & \num{18551518503} \\
Instructions     & \num{12968103304} \\
IPC              & 0.70 \\
Cache references & \num{691719328}   \\
Cache misses     & \num{482020663} (69.7\%) \\
L1d load misses  & \num{134692327}   \\
LLC load misses  & \num{18685337}    \\
dTLB load misses & \num{8565404}     \\
Branch misses    & \num{185437635} (9.3\% of \num{1998387729}) \\
\bottomrule
\end{tabular}
\end{table}

Per-operation: $\sim$1106 cycles, $\sim$773 instructions, $\sim$8.0
L1d misses, $\sim$1.1 LLC misses, $\sim$0.5 dTLB misses, $\sim$11
branch mispredicts.  The 0.70 IPC indicates that $\sim$30\% of
potential throughput is lost to cache and branch stalls.

\subsection{Potential Throughput Improvements}
\label{sec:future_improvements}

\subsubsection{A. Enable Superpages (Already Implemented)}

With 2\,MiB superpages (up to 510 pages per superpage), the 78K leaf
pages at $N{=}16\text{M}$ fit in $\sim$153 superpages.  The outer tree
shrinks from 78K children (height~1 with a 4\,KiB root inode) to 153
children (root inode fits in a few hundred bytes, entirely in L1).
Page-level internal nodes within each superpage are co-located in the
same 2\,MiB region, improving TLB and spatial locality.

The superpage infrastructure is already implemented
(\texttt{mt\_hierarchy\_init\_superpage}); enabling it for the fence
variant is a configuration change.

\paragraph{Expected improvement:} 15--25\% at $N > 4\text{M}$
(eliminates most outer-tree overhead, reduces TLB misses).

\subsubsection{B. Batched Prefetch Pipelining}

For bulk workloads, sort incoming keys and group by target leaf.
While inserting key[$i$] into its CL leaf, prefetch the page header
for key[$i{+}1$]'s target leaf.  This completely hides the 44\%
page-header miss by overlapping it with the previous key's CL leaf
insert ($\sim$280 cycles of useful work --- more than enough to hide
even a DRAM miss).

The batch insert API already exists (\texttt{matryoshka\_insert\_batch})
but does not currently pipeline prefetches across keys.

\paragraph{Expected improvement:} 20--30\% for batch workloads
(eliminates the dominant page-header cold miss entirely).

\subsubsection{C. Branchless Binary Search in Outer Inodes}

Replace the data-dependent \texttt{cmp/jle} branches in
\texttt{mt\_inode\_search}'s binary search loop with CMOV-based
branchless updates:
\begin{lstlisting}[language=C, xleftmargin=2em]
while (lo < hi) {
    int mid = lo + (hi - lo) / 2;
    int go_right = (keys[mid] <= key);
    lo = go_right ? mid + 1 : lo;   /* CMOV */
    hi = go_right ? hi : mid;       /* CMOV */
}
\end{lstlisting}
This eliminates $\sim$4 branch mispredicts per insert $\times$
$\sim$15 cycle penalty $= \sim$60 cycles saved per operation.  The
CMOV version is also more amenable to the CPU's load speculation since
there are no mispredicted branches to flush.

\paragraph{Expected improvement:} 5--8\%
(reduces branch misprediction from 9.3\% to $\sim$6\%).

\subsubsection{D. FAST-Layout Outer Inodes}

The FAST paper~[\ref{ref:fast}] describes a cache-line-blocked layout
for internal nodes that reorganises keys so each binary search step
lands on a different cache line, with the tree structure matching the
cache hierarchy.  Applied to the 339-key outer inodes:

\begin{itemize}[nosep]
  \item Level~0 (root line): 1 key, splits the node in half
  \item Level~1: 2 keys, one per half
  \item \ldots until cache-line-sized blocks of $\sim$15 keys
\end{itemize}

Each level's cache line can be prefetched one level ahead.  The
current sorted-order layout requires $\sim$9 random cache line
accesses within the 4\,KiB inode; the FAST layout converts this to a
BFS descent where each level is a single cache line, enabling
systematic prefetching.

\paragraph{Expected improvement:} 10--15\%
(reduces outer inode search from $\sim$240 to $\sim$150 cycles by
eliminating $\sim$2 serial cache line misses).

\subsubsection{E. Compile-Time Strategy Specialisation}

The fence key fast path currently checks
\texttt{hier->cl\_strategy == MT\_CL\_STRAT\_FENCE} at runtime on
every insert.  Compile-time specialisation (via \texttt{\_\_attribute\_\_((flatten))}
or template instantiation) would:
\begin{itemize}[nosep]
  \item Eliminate runtime strategy branches ($\sim$2--3 per insert)
  \item Allow the compiler to inline \texttt{page\_find\_leaf} into
        \texttt{mt\_page\_insert}, fusing the CL leaf resolution with
        the insert
  \item Reduce function call overhead ($\sim$20 cycles for
        \texttt{callq}/\texttt{retq} + register save/restore)
\end{itemize}

\paragraph{Expected improvement:} 3--5\%
(reduces instruction count and eliminates function call overhead).

\subsubsection{Summary}

\begin{table}[H]
\centering
\caption{Potential throughput improvements for
  \texttt{rand\_insert} at $N{=}16\text{M}$.}
\label{tab:future_improvements}
\begin{tabular}{llrl}
\toprule
\textbf{Approach} & \textbf{Bottleneck addressed}
  & \textbf{Est.\ gain} & \textbf{Effort} \\
\midrule
Superpages (config change) & Outer tree height + TLB & 15--25\% & Low \\
Batched prefetch pipelining & Page header cold miss (44\%) & 20--30\% & Medium \\
Branchless inode binary search & Branch misprediction (9.3\%) & 5--8\% & Small \\
FAST-layout outer inodes & Inode search cache misses & 10--15\% & Large \\
Compile-time specialisation & Runtime dispatch overhead & 3--5\% & Medium \\
\bottomrule
\end{tabular}
\end{table}

Approaches A--C are independent and composable; their improvements
should be roughly additive.  Approach D (FAST layout) is orthogonal
to A--C but requires significant restructuring of the outer inode
format.  The combined ceiling from A+B+C is $\sim$40--60\%
improvement for batch workloads, potentially bringing matryoshka to
$\sim$500\,ns/op at $N{=}16\text{M}$ --- competitive with libart's
384\,ns/op.

\subsection{Implementation Results: Approaches A--C}
\label{sec:abc_results}

Approaches A (superpages + fence keys), B (batched prefetch pipelining),
and C (branchless binary search in outer inodes) have been implemented
and benchmarked.

\subsubsection{C. Branchless Binary Search --- Implementation}

The \texttt{mt\_inode\_search} binary search loop (lines~89--102 of
\texttt{inode.c}) was converted from a data-dependent \texttt{if/else}
branch to arithmetic bit-masking:

\begin{verbatim}
int cmp  = (keys[mid] <= key);   // 0 or 1
int mask = -cmp;                 // 0x00000000 or 0xFFFFFFFF
lo += ((mid + 1) - lo) & mask;   // lo = mid+1 if cmp, else unchanged
hi += (mid - hi) & ~mask;        // hi = mid if !cmp, else unchanged
\end{verbatim}

Disassembly confirms the compiler emits \texttt{setle} + \texttt{cmovle}
+ \texttt{and} instead of conditional jumps in the inner loop body.  The
only branch is the well-predicted \texttt{while (lo < hi)} loop exit.

\subsubsection{B. Batched Prefetch Pipelining --- Implementation}

The \texttt{matryoshka\_insert\_batch} function was enhanced with two
optimisations:

\begin{enumerate}
\item \textbf{Sibling-advance fast path.}  When sorted keys cross a
  leaf boundary, the code checks whether the next key falls into the
  next sibling child within the same parent inode.  Since outer inodes
  hold up to 340 children, this skips the full outer-tree re-walk for
  $\sim$339/340 of leaf transitions.

\item \textbf{Eager next-sibling prefetch.}  At the start of each
  leaf group, the code prefetches the next sibling leaf's page header
  and CL root slot via the parent's tagged child pointer.  The
  $\sim$400--855 inserts for the current leaf provide ample latency
  to hide the DRAM fetch.
\end{enumerate}

These optimisations apply only to the batch insert API; single-key
\texttt{matryoshka\_insert} is unchanged.

\subsubsection{A. Superpages + Fence Keys --- Implementation}

A new hierarchy initialiser \texttt{mt\_hierarchy\_init\_fence\_sp}
combines fence keys (\texttt{cl\_strategy = MT\_CL\_STRAT\_FENCE})
with 2\,MiB superpage allocation (\texttt{use\_superpages = true}).
The two features compose naturally: the superpage code delegates to
page-level functions that already check \texttt{cl\_strategy}.

\subsubsection{Benchmark Results: Approaches A--C}

\begin{table}[H]
\centering
\caption{Throughput (Mop/s) for \texttt{rand\_insert} after implementing
  approaches A--C.  All matryoshka variants include branchless binary search
  (approach C).  Batch prefetch (B) only affects \texttt{insert\_batch}.}
\label{tab:abc_throughput}
\begin{tabular}{lS[table-format=1.2]S[table-format=1.2]S[table-format=4.0]r}
\toprule
\textbf{Variant} & {$N{=}4\text{M}$} & {$N{=}16\text{M}$}
  & {ns/op (16M)} & \textbf{vs.\ ART} \\
\midrule
Matryoshka (baseline)     & 1.87 & 1.11 &  901 & $0.54{\times}$ \\
\rowcolor{lightgray!20}
Matryoshka + fence        & 1.32 & 0.98 & 1020 & $0.48{\times}$ \\
Matryoshka + fence + SP   & 1.17 & 0.94 & 1064 & $0.46{\times}$ \\
\midrule
libart (ART)              & 2.52 & 2.06 &  486 & $1.00{\times}$ \\
TLX btree\_set            & 1.20 & 0.83 & 1205 & $0.40{\times}$ \\
Abseil btree\_set         & 1.41 & 0.97 & 1031 & $0.47{\times}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Hardware Counter Impact}

\begin{table}[H]
\centering
\caption{Selected hardware counters for \texttt{rand\_insert} at
  $N{=}16\text{M}$.  Cache-miss ratio = cache-misses / cache-references.}
\label{tab:abc_hwcounters}
\begin{tabular}{lrrrr}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Fence} & \textbf{Fence+SP}
  & \textbf{libart} \\
\midrule
Cache-miss ratio   & 76.9\% & 76.1\% & 53.3\% & 64.8\% \\
\bottomrule
\end{tabular}
\end{table}

Key findings from the hardware counter analysis:

\begin{itemize}
\item \textbf{Cache miss rate:} Superpages reduce the cache-miss ratio
  from $\sim$76\% (fence) to $\sim$53\% (fence+SP) at $N{=}16\text{M}$.
  The 2\,MiB TLB entries and spatial co-location of pages within
  superpages substantially improve LLC hit rates.

\item \textbf{Branchless binary search:} The \texttt{setle}/\texttt{cmovle}
  codegen eliminates data-dependent branches in the outer inode search.
  For inodes with $>$64 keys (where the AVX2 linear scan falls back to
  binary search), this saves $\sim$4 mispredictions per search at
  $\sim$15 cycles each.

\item \textbf{Superpage throughput:} Despite the dramatic cache-miss
  reduction, the fence+SP variant shows slightly lower throughput than
  plain fence keys for random inserts.  The additional indirection through
  the superpage page-level B+ sub-tree (finding the correct 4\,KiB page
  within the 2\,MiB superpage) adds instructions that offset the cache
  improvement.

\item \textbf{Known limitation:} Large-scale random deletion with
  superpages triggers a pre-existing infinite loop in the superpage
  rebalance code (\texttt{rebalance\_sp}).  The issue is in the
  superpage merge path when multiple superpages need to be consolidated.
  Delete-heavy workloads (rand\_delete, mixed, search\_after\_churn)
  are not available for the fence+SP variant at $N{\geq}2\text{M}$.
\end{itemize}

\subsubsection{Progress Toward libart Parity}

At $N{=}16\text{M}$ random insert, the current standing:

\begin{itemize}
\item \textbf{libart (ART):} $\sim$2.06\,Mop/s (486\,ns/op)
\item \textbf{Matryoshka + fence (with branchless search):}
  $\sim$0.98\,Mop/s (1020\,ns/op) --- 2.1$\times$ slower than ART
\item \textbf{Matryoshka + fence + superpages:}
  $\sim$0.94\,Mop/s (1064\,ns/op) --- cache misses cut from 76\% to
  53\%, but superpage overhead offsets the gain
\end{itemize}

The gap remains substantial (2$\times$).  The primary remaining
bottleneck is the CL sub-tree traversal within each leaf page, which
requires 2--3 serial cache-line loads.  The approaches implemented here
(branchless search, batch prefetch, superpages) address the outer-tree
descent but cannot reduce the intra-page serial dependency chain.
Closing the gap would require restructuring the page-level layout
(e.g., FAST-layout inodes, or flattening the CL sub-tree to a sorted
array with SIMD search for small pages).

\subsection{Future: Variable-Length Keys}

The current 4-byte \texttt{int32\_t} key format could be extended to
variable-length keys by storing key offsets or using indirection within
CL sub-nodes.  This would broaden applicability at the cost of some
cache efficiency.

%% ═══════════════════════════════════ References ════════════════════════════
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{enumerate}[label={[\arabic*]}, nosep, leftmargin=2em]
  \item \label{ref:fast}
        C.~Kim, J.~Chhugani, N.~Satish, E.~Sedlar, A.~D.~Nguyen,
        T.~Kaldewey, V.~W.~Lee, S.~A.~Brandt, and P.~Dubey.
        \emph{FAST: Fast Architecture Sensitive Tree Search on Modern
        CPUs and GPUs.}  SIGMOD~'10, pp.~339--350, 2010.
  \item \label{ref:btree}
        R.~Bayer and E.~McCreight.
        \emph{Organization and Maintenance of Large Ordered Indexes.}
        Acta Informatica, 1(3):173--189, 1972.
  \item \label{ref:art}
        V.~Leis, A.~Kemper, and T.~Neumann.
        \emph{The Adaptive Radix Tree: ARTful Indexing for Main-Memory
        Databases.}  ICDE~'13, pp.~38--49, 2013.
  \item \label{ref:cssbtree}
        J.~Rao and K.~A.~Ross.
        \emph{Making B+-Trees Cache Conscious in Main Memory.}
        SIGMOD~'00, pp.~475--486, 2000.
\end{enumerate}

\end{document}
