%% report_template.tex.j2 -- Jinja2 LaTeX template for the matryoshka
%% comparative benchmark report.
%%
%% Custom Jinja2 delimiters for LaTeX compatibility:
%%   BLOCK, VAR, and comment delimiters with backslash prefix.
%%
\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=matryoshkacolor,urlcolor=matryoshkacolor,citecolor=matryoshkacolor]{hyperref}
\usepackage{fancyhdr}
\usepackage{siunitx}
\usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{float}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{listings}
\usepackage{caption}
\usepackage{colortbl}

%% ── Colours ─────────────────────────────────────────────────────────────────
\definecolor{matryoshkacolor}{RGB}{31,119,180}
\definecolor{stdsetcolor}{RGB}{214,39,40}
\definecolor{tlxcolor}{RGB}{148,103,189}
\definecolor{artcolor}{RGB}{255,127,14}
\definecolor{abseilcolor}{RGB}{44,160,44}
\definecolor{rowhl}{RGB}{220,230,242}

%% ── Page style ──────────────────────────────────────────────────────────────
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Matryoshka B+ Tree Benchmark Report}
\fancyhead[R]{\small \VAR{date}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\lstset{basicstyle=\ttfamily\small, breaklines=true, frame=single, columns=fullflexible}
\sisetup{group-separator={,}, group-minimum-digits=4}

\begin{document}

%% ═══════════════════════════════════ Title Page ═════════════════════════════
\begin{titlepage}
\centering
\vspace*{3cm}
{\Huge\bfseries Matryoshka B+ Tree:\\[0.3em]
Insert/Delete Performance Report\par}
\vspace{1.5cm}
{\Large Comparative Benchmark Results\par}
\vspace{2cm}
{\large \VAR{date}\par}
\vspace{3cm}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
CPU       & \VAR{sysinfo.cpu} \\
L1d Cache & \VAR{sysinfo.l1d} \\
L2 Cache  & \VAR{sysinfo.l2} \\
L3 Cache  & \VAR{sysinfo.l3} \\
Kernel    & \VAR{sysinfo.kernel} \\
Page Size & \VAR{sysinfo.page_size} \\
\bottomrule
\end{tabular}
\vfill
{\small Generated by \texttt{bench\_compare} --- matryoshka project\par}
\end{titlepage}

%% ═══════════════════════════════════ TOC ════════════════════════════════════
\tableofcontents
\newpage

%% ═══════════════════════════════════ Introduction ══════════════════════════
\section{Introduction}

This report evaluates the \textcolor{matryoshkacolor}{\textbf{matryoshka}}
B+ tree --- a B+ tree whose page-sized leaf nodes contain nested B+ sub-trees
of cache-line-sized (64\,B) sub-nodes, with SIMD-accelerated search at every
level --- against several tree and ordered-map libraries on \emph{insert-heavy}
and \emph{delete-heavy} workloads.  Goals:
\begin{enumerate}[nosep]
  \item Quantify the modification throughput gap across dataset sizes
        (\num{65536} to \num{16777216} keys).
  \item Identify micro-architectural bottlenecks (cache misses, TLB
        pressure, branch misprediction) that explain the differences.
\end{enumerate}
All measurements use \texttt{clock\_gettime(CLOCK\_MONOTONIC)}.  Results
are reported as \si{Mop/s} and \si{ns/op}.

%% ═══════════════════════════════════ Libraries ═════════════════════════════
\section{Library Descriptions}

\begin{table}[H]
\centering
\caption{Libraries under test.}
\label{tab:libraries}
\begin{tabular}{lll}
\toprule
\textbf{Name} & \textbf{Label} & \textbf{Description} \\
\midrule
\BLOCK{for lib in libraries}
\texttt{\VAR{lib.name}} & \VAR{lib.label} & \VAR{lib.description} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

%% ═══════════════════════════════════ Workloads ═════════════════════════════
\section{Workload Descriptions}

\begin{table}[H]
\centering
\caption{Benchmark workloads.}
\label{tab:workloads}
\begin{tabular}{lp{10cm}}
\toprule
\textbf{Workload} & \textbf{Description} \\
\midrule
\texttt{seq\_insert}
  & Insert $N$ keys in ascending order. Exercises append paths. \\
\texttt{rand\_insert}
  & Insert $N$ unique keys in random order. Stresses leaf splits. \\
\texttt{ycsb\_a}
  & 95\% insert / 5\% search. Write-dominated OLTP model. \\
\texttt{rand\_delete}
  & Bulk-load $N$ sorted keys, delete all in random order. \\
\texttt{mixed}
  & Bulk-load $N$ keys, then $N$ alternating insert/delete ops. \\
\texttt{ycsb\_b}
  & Bulk-load $N$ keys, then 50\% delete / 50\% search. \\
\texttt{search\_after\_churn}
  & Bulk-load $N$ keys, $N/2$ mixed churn (untimed), then
    \num{5000000} random predecessor searches. \\
\bottomrule
\end{tabular}
\end{table}

%% ═══════════════════════════════════ Insert-Heavy Results ══════════════════
\section{Results: Insert-Heavy Workloads}

\subsection{Sequential Insert}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_seq_insert}}
\caption{Sequential insert throughput (Mop/s).}
\label{fig:bar_seq_insert}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_seq_insert}}
\caption{Sequential insert scaling.}
\label{fig:scale_seq_insert}
\end{figure}

\subsection{Random Insert}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_rand_insert}}
\caption{Random insert throughput (Mop/s).}
\label{fig:bar_rand_insert}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_rand_insert}}
\caption{Random insert scaling.}
\label{fig:scale_rand_insert}
\end{figure}

\subsection{YCSB-A (95\% Insert / 5\% Search)}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_ycsb_a}}
\caption{YCSB-A throughput (Mop/s).}
\label{fig:bar_ycsb_a}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_ycsb_a}}
\caption{YCSB-A scaling.}
\label{fig:scale_ycsb_a}
\end{figure}

%% ═══════════════════════════════════ Delete-Heavy Results ══════════════════
\section{Results: Delete-Heavy Workloads}

\subsection{Random Delete}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_rand_delete}}
\caption{Random delete throughput (Mop/s).}
\label{fig:bar_rand_delete}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_rand_delete}}
\caption{Random delete scaling.}
\label{fig:scale_rand_delete}
\end{figure}

\subsection{Mixed Insert/Delete}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_mixed}}
\caption{Mixed insert/delete throughput (Mop/s).}
\label{fig:bar_mixed}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_mixed}}
\caption{Mixed insert/delete scaling.}
\label{fig:scale_mixed}
\end{figure}

\subsection{YCSB-B (50\% Delete / 50\% Search)}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_ycsb_b}}
\caption{YCSB-B throughput (Mop/s).}
\label{fig:bar_ycsb_b}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_ycsb_b}}
\caption{YCSB-B scaling.}
\label{fig:scale_ycsb_b}
\end{figure}

%% ═══════════════════════════════════ Search After Churn ════════════════════
\section{Results: Search After Churn}

The \texttt{search\_after\_churn} workload measures pure search
throughput on a tree that has undergone insert/delete churn,
isolating search performance from modification cost.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_bar_search_after_churn}}
\caption{Search throughput after churn (Mop/s).}
\label{fig:bar_search_after_churn}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_scale_search_after_churn}}
\caption{Search-after-churn scaling.}
\label{fig:scale_search_after_churn}
\end{figure}

%% ═══════════════════════════════════ Hardware Counters ═════════════════════
\section{Hardware Counter Analysis}

\BLOCK{if chart_perf}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\VAR{chart_perf}}
\caption{Hardware counters: dTLB miss rate, LLC miss rate, IPC,
         branch misprediction rate.}
\label{fig:perf_counters}
\end{figure}

\subsection{dTLB Miss Rate}
Matryoshka's arena allocator places leaf nodes in contiguous 2\,MiB
superpage-aligned regions.  At $N = \num{\VAR{largest_n}}$, matryoshka's
dTLB miss rate is \VAR{matryoshka_dtlb_miss_rate} per \num{1000} ops,
versus \VAR{stdset_dtlb_miss_rate} for \texttt{std::set}.  Red-black
tree pointer chasing touches a new TLB entry per level; matryoshka
confines each leaf search to a single 4\,KiB page.

\subsection{LLC Miss Rate}
Matryoshka packs up to 855 keys per 4\,KiB page using a nested B+
sub-tree of cache-line sub-nodes.  \texttt{std::set} requires one
40--48\,B heap node per key.  At $N = \num{\VAR{largest_n}}$:
\VAR{matryoshka_llc_miss_rate} LLC misses/\num{1000}\,ops (matryoshka)
vs.\ \VAR{stdset_llc_miss_rate} (\texttt{std::set}).

\subsection{IPC}
SIMD search at every level (CL leaves, CL internals, outer internal
nodes) achieves IPC of \VAR{matryoshka_ipc} via pipelined
\texttt{\_mm\_cmpgt\_epi32}/\texttt{\_mm\_movemask\_ps} without
data-dependent branches.  Insert and delete paths also benefit from
SIMD navigation through the CL sub-tree to locate the target sub-node.

\subsection{Branch Misprediction}
SIMD mask arithmetic replaces conditional branches during search,
yielding near-zero misprediction.  Modification operations navigate the
CL sub-tree using the same SIMD search, with only the final CL leaf
insert/delete using scalar \texttt{memmove} on $\le 14$ keys.
\BLOCK{else}
Hardware counter data was not collected for this run.  Re-run with
\texttt{perf} support enabled (omit \texttt{-{}-no-perf}) to populate
this section with dTLB miss rates, LLC miss rates, IPC, and branch
misprediction data.
\BLOCK{endif}

%% ═══════════════════════════════════ Profiling ═════════════════════════════
\section{Profiling: Hot Functions}

\BLOCK{if profile_functions}
\begin{table}[H]
\centering
\caption{Top functions (\texttt{perf record}, rand\_insert, $N{=}\num{1048576}$).}
\label{tab:profile}
\begin{tabular}{rll}
\toprule
\textbf{\% Overhead} & \textbf{Function} & \textbf{Source} \\
\midrule
\BLOCK{for fn in profile_functions}
\num{\VAR{fn.overhead}}\% & \texttt{\VAR{fn.name}} & \texttt{\VAR{fn.source}} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

The hot functions are the page-level sub-tree operations:
\texttt{mt\_page\_insert} and \texttt{mt\_page\_delete} navigate the CL
sub-tree via SIMD search, then perform a scalar insert or delete within
a single 64\,B cache-line sub-node.  CL sub-node splits and merges
occur only on overflow or underflow.
\BLOCK{else}
Profiling data was not collected for this run.  Re-run with
\texttt{perf} support enabled (omit \texttt{-{}-no-perf}) to populate
this section.
\BLOCK{endif}

%% ═══════════════════════════════════ Cache-Miss Attribution ═══════════════
\section{Cache-Miss Attribution Analysis}

\BLOCK{if cache_miss_functions}
This section attributes cache misses to individual functions using
\texttt{perf record -e cache-misses}.  The data reveals \emph{where}
in the code the processor stalls waiting for memory, complementing
the aggregate hardware counters in Section~7.

\begin{table}[H]
\centering
\caption{Top functions by cache-miss contribution
  (matryoshka, rand\_insert, $N{=}\num{4194304}$).}
\label{tab:cache_miss_attr}
\begin{tabular}{rll}
\toprule
\textbf{\% Cache Misses} & \textbf{Function} \\
\midrule
\BLOCK{for fn in cache_miss_functions}
\num{\VAR{fn.pct}}\% & \texttt{\VAR{fn.name}} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

\subsection{Root Cause: CL Sub-Node Cache-Line Jumps}

The dominant cache-miss source is the CL sub-tree navigation within
each 4\,KiB page leaf.  When \texttt{page\_find\_leaf} descends the
CL sub-tree, each level accesses a different 64\,B slot within the
page.  These slots are not adjacent --- slot indices are assigned by
a bitmap allocator, so the root CL inode, its child CL inode, and
the target CL leaf typically reside on non-contiguous cache lines.

The hardware prefetcher cannot predict these indirect jumps (each
CL internal node stores child slot indices that must be read before
the next cache line address is known).  This creates a dependent
chain of cache misses: \emph{read child index $\rightarrow$ compute
address $\rightarrow$ miss on that address $\rightarrow$ repeat}.

At $N{=}4\text{M}$ with random inserts, the 4\,KiB pages are spread
across a $\sim$20\,MiB working set, exceeding L2 but fitting in L3.
Each page access brings the header cache line into L1, but the
CL sub-nodes deeper in the page have been evicted since the last
visit to that page.

\subsection{Optimisation: Software Prefetching}

Software prefetch hints (\texttt{\_\_builtin\_prefetch}) have been
added at three levels:

\begin{description}[style=nextline, nosep, leftmargin=1em]
  \item[\texttt{page\_find\_leaf} (CL sub-tree descent)]
    After reading the page header's \texttt{root\_slot}, the root
    CL node's cache line is prefetched before the loop begins.
    Within each loop iteration, after determining the child slot
    index from \texttt{cl\_inode\_search}, the child's cache line
    is prefetched before the next iteration accesses it.  This
    converts the dependent miss chain into overlapped prefetch +
    computation.

  \item[\texttt{find\_leaf} (outer B+ tree descent)]
    After \texttt{mt\_inode\_search} determines the child pointer,
    the first cache line of the child node is prefetched.  For
    internal-to-internal transitions, this warms the next inode's
    header and first keys.  For the last level (internal-to-leaf),
    this warms the page header so that \texttt{page\_find\_leaf}
    starts with the header in L2.

  \item[\texttt{mt\_inode\_search} (binary search in outer inodes)]
    For nodes exceeding the SIMD linear scan threshold, the binary
    search prefetches the midpoints of both candidate halves before
    each comparison.  This converts the $O(\log N)$ dependent miss
    chain of binary search into a pipelined prefetch sequence.
\end{description}

All prefetches use \texttt{locality=1} (L2 hint), appropriate for
data that will be used once in the near future but is unlikely to
be reused before eviction.

\subsection{Optimisation: Pointer Tagging for CL Root Prefetch}

Instruction-level profiling revealed that the initial software prefetching
approach had insufficient lead time: the CL root prefetch inside
\texttt{page\_find\_leaf} was issued \emph{after} loading the page header
(to read \texttt{root\_slot}), leaving only $\sim$5\,ns of computation
before the CL root access---far less than the $\sim$100\,ns needed for
an L3 miss to resolve.

To address this, leaf metadata is embedded in the low bits of child pointers
in the outer B+ tree's internal nodes.  All node pointers are 4096-byte
aligned, providing 12 guaranteed-zero low bits.  The encoding is:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Bits} & \textbf{Field} & \textbf{Range} \\
\midrule
0--5  & \texttt{root\_slot}  & 1--63 (CL sub-tree root slot index) \\
6--8  & \texttt{sub\_height} & 0--7  (CL sub-tree height) \\
9--11 & \textit{reserved}    & (available for future use, e.g.\ fullness counts) \\
\bottomrule
\end{tabular}
\end{center}

With this scheme, \texttt{find\_leaf} can extract \texttt{root\_slot} from
the tagged child pointer \emph{without} reading the page header, enabling
it to issue \emph{two} prefetches simultaneously at the last outer-tree
level:

\begin{enumerate}[nosep]
  \item The page header cache line (slot 0), as before;
  \item The CL root cache line (\texttt{slots[root\_slot $-$ 1]}),
        computed from the tag.
\end{enumerate}

Both prefetches issue at the same time, overlapping with whatever inode
search computation follows.  Tags are maintained at leaf creation
(bulk-load, split) and updated after each insert or delete that may
change \texttt{root\_slot} or \texttt{sub\_height}.  Every child pointer
read goes through an \texttt{mt\_untag()} mask operation (a single AND
instruction) to strip the low 12 bits before dereferencing.  Stale tags
on cold paths (rebalance) merely cause a wasted prefetch---never a
correctness issue.
\BLOCK{else}
Cache-miss attribution data was not collected for this run.  Re-run
with \texttt{perf} support enabled to populate this section with
per-function cache-miss breakdowns.
\BLOCK{endif}

%% ═══════════════════════════════════ Results Table ═════════════════════════
\section{Detailed Results Table}

Matryoshka rows highlighted in \colorbox{rowhl}{blue}.

\begin{longtable}{llrS[table-format=4.2]S[table-format=6.1]}
\caption{Full benchmark results.\label{tab:results}} \\
\toprule
\textbf{Library} & \textbf{Workload} & \textbf{N} &
  {\textbf{Mop/s}} & {\textbf{ns/op}} \\
\midrule
\endfirsthead
\caption[]{Full benchmark results (continued).} \\
\toprule
\textbf{Library} & \textbf{Workload} & \textbf{N} &
  {\textbf{Mop/s}} & {\textbf{ns/op}} \\
\midrule
\endhead
\midrule
\multicolumn{5}{r}{\emph{Continued on next page}} \\
\bottomrule
\endfoot
\bottomrule
\endlastfoot
\BLOCK{for r in results}
\BLOCK{if r.library == 'matryoshka'}
\rowcolor{rowhl}
\BLOCK{endif}
\texttt{\VAR{r.library}} & \texttt{\VAR{r.workload}} &
  \num{\VAR{r.n}} & \VAR{r.mops} & \VAR{r.ns_per_op} \\
\BLOCK{endfor}
\end{longtable}

%% ═══════════════════════════════════ Analysis ══════════════════════════════
\section{Analysis and Diagnosis}

\subsection{Benchmark Access Patterns}

The seven workloads exercise distinct access patterns that interact
differently with each data structure's memory layout:

\begin{description}[style=nextline, nosep, leftmargin=1em]
  \item[\texttt{seq\_insert}]
    Keys arrive in ascending order (1, 3, 5, \ldots).  Structures with
    append-optimised paths benefit: B-trees fill leaves left-to-right
    with no splits until full; red-black trees rebalance but maintain
    temporal locality in the allocator.  ART builds monotonically deeper
    paths in byte-order.  Matryoshka benefits from sequential CL sub-node
    filling within each page, but the nested sub-tree overhead (navigating
    CL internals) makes each insert more expensive than a simple sorted-array
    append.

  \item[\texttt{rand\_insert}]
    A Fisher--Yates shuffle of $[0, N)$ scaled to odd values.  Every
    insert touches a random leaf, maximising cache pressure.  This is the
    workload most sensitive to node size and memory layout: wide B-tree
    leaves amortise random access; pointer-chasing structures
    (\texttt{std::set}) incur a cache miss per tree level; ART's
    fixed-depth (4-byte key $\Rightarrow$ $\le 4$ levels) limits misses.

  \item[\texttt{rand\_delete}]
    Bulk-loads $N$ sorted keys, then deletes all in shuffled order.
    The bulk-load gives every structure its optimal starting layout.
    Random deletion stresses rebalancing: red-black trees do O(1)
    rotations; B-trees \texttt{memmove} within leaves and occasionally
    merge or redistribute; matryoshka merges CL sub-nodes within a page,
    with page-level merges only on underflow.

  \item[\texttt{mixed}]
    Alternating insert (new key beyond current max) and delete (random
    existing key) on a pre-loaded tree.  This creates a steady-state
    workload where the tree size fluctuates around $N$.  Structures
    that handle interleaved modifications without excessive rebalancing
    perform well.

  \item[\texttt{ycsb\_a} (95\% insert / 5\% search)]
    Simulates a write-dominated OLTP workload.  Inserts are sequential
    (monotonically increasing keys), so append-path efficiency matters.
    The 5\% predecessor searches touch recently-inserted regions, favouring
    structures with good temporal locality in their leaf layer.

  \item[\texttt{ycsb\_b} (50\% delete / 50\% search)]
    Deletes from a pre-loaded tree interleaved with random predecessor
    searches.  The shrinking tree tests whether structures maintain
    search efficiency as occupancy drops and structural changes
    (merges, rebalancing) occur.

  \item[\texttt{search\_after\_churn}]
    Pure search workload on a tree that has undergone insert/delete churn
    (untimed).  \num{5000000} random predecessor searches isolate search
    throughput from modification cost.  This workload is most sensitive
    to in-leaf search cost and memory layout, not structural operations.
    Note: ART lacks native predecessor search; its wrapper falls back to
    point lookup, giving it an advantage on hit rate but testing a
    different operation.
\end{description}

All workloads use 4-byte \texttt{int32\_t} keys encoded as odd values
($2i{+}1$), generated by xorshift64 with fixed seeds for reproducibility.

\subsection{Matryoshka: Nested Sub-Tree Tradeoffs}

Each insert or delete navigates the page-level CL sub-tree (2--3 SIMD
comparisons on 12--15 keys per level) to a target CL leaf, then performs
a scalar \texttt{memmove} of at most 14 keys within that 64\,B
cache-line sub-node.  The cost per modification is
$O(h_s \times b)$ where $h_s \le 2$ is the sub-tree height and
$b = 15$ is the CL leaf capacity---roughly 30--45 key touches, all
within a single 4\,KiB page.

CL sub-node splits and merges occur only when a CL leaf overflows
(15 keys) or underflows ($< 7$ keys).  Page-level splits occur only
when all 63 CL slots are exhausted ($\sim$855 keys/page).

The nested design adds a constant overhead per operation compared to
flat sorted-array B-tree leaves, where a single \texttt{memmove}
suffices.  At $N{=}\num{1048576}$: matryoshka achieves
\VAR{mat_rand_insert_mops} Mop/s on random insert, vs.\
\VAR{abseil_rand_insert_mops} (Abseil) and
\VAR{tlx_rand_insert_mops} (TLX).

However, SIMD search through the CL sub-tree is used during both
search \emph{and} the navigation phase of insert/delete.  This yields
search-after-churn throughput of \VAR{mat_search_after_churn_mops}
Mop/s at $N{=}\num{1048576}$.  The key advantage is that modifications
touch a single cache-line sub-node rather than shifting an entire
sorted leaf array.

\subsection{Detailed Comparison: std::set (Red-Black Tree)}

\texttt{std::set} uses a balanced binary search tree with one
heap-allocated node per key (40--48\,B on 64-bit systems: two child
pointers, parent pointer, colour bit, key, allocator overhead).

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{stdset_rand_insert_mops_large}
Mop/s (random insert) and \VAR{stdset_rand_delete_mops_large} Mop/s
(random delete)---the slowest of all libraries tested.
Each operation traverses $O(\log_2 N)$ levels with a pointer
dereference (and likely cache miss) at every level.  The 40--48\,B node
size means $\sim$1 useful key per cache line, so every level is a
full cache miss for large $N$.

\paragraph{Sequential insert.}
At \VAR{stdset_seq_insert_mops_large} Mop/s
($N{=}\num{\VAR{largest_n}}$), sequential insert is only modestly
better than random because the allocator provides some temporal
locality, but the red-black tree still requires $O(\log N)$ pointer
chases and rotations.

\paragraph{Search.}
Search-after-churn: \VAR{stdset_search_after_churn_mops_large} Mop/s.
Binary search through $\log_2 N \approx 24$ levels of pointer chasing
is inherently cache-unfriendly.  \texttt{std::set} has no mechanism
for SIMD-accelerated search or cache-line-aware layout.

\paragraph{Scaling.}
\texttt{std::set} shows the steepest throughput degradation from small
to large $N$ (random insert scales
\VAR{scale_stdset_rand_insert}$\times$ from $N{=}\num{\VAR{smallest_n}}$
to $N{=}\num{\VAR{largest_n}}$) because the working set of
pointer-chased nodes quickly exceeds cache capacity.

\subsection{Detailed Comparison: TLX btree\_set}

TLX implements a B+ tree with sorted-array leaves.  Leaf capacity is
typically 64--128 keys (depends on template parameters and key size).
Internal nodes use sorted arrays of separator keys with binary search.

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{tlx_rand_insert_mops_large} Mop/s
(random insert) and \VAR{tlx_rand_delete_mops_large} Mop/s (random
delete).  Each leaf insert is a binary search followed by a
\texttt{memmove} of the leaf's sorted array.  The average shift is
$B/2 \approx 32$--64 keys per insert, but the entire operation stays
within one or two cache lines for small leaves.

\paragraph{Search.}
Search-after-churn: \VAR{tlx_search_after_churn_mops_large} Mop/s.
TLX uses scalar binary search within leaves, which incurs
$\lceil\log_2 B\rceil$ comparisons with data-dependent branches.
This is slower than SIMD linear scan for the same leaf size.

\paragraph{Sequential insert.}
\VAR{tlx_seq_insert_mops_large} Mop/s.  The B+ tree append path is
efficient: new keys land at the rightmost leaf with minimal shifting,
and splits propagate only when the leaf is full.

\paragraph{Comparison to matryoshka.}
TLX's flat sorted-array leaves have a lower constant factor per
insert (one \texttt{memmove} vs.\ CL sub-tree navigation), but
matryoshka's wider pages (855 keys vs.\ $\sim$128) reduce the outer
tree height and number of leaf splits.  At large $N$, the outer-tree
traversal cost dominates, and matryoshka's SIMD-accelerated outer
internal node search closes the gap.

\subsection{Detailed Comparison: Abseil btree\_set}

Abseil's B-tree uses a similar sorted-array design to TLX but with
different node sizes and allocation strategies.  Leaf nodes hold up
to $\sim$256 keys in a single sorted array.

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{abseil_rand_insert_mops_large}
Mop/s (random insert) and \VAR{abseil_rand_delete_mops_large} Mop/s
(random delete).  The wider leaves mean fewer tree levels and splits,
but each \texttt{memmove} within a leaf shifts more keys on average
($B/2 \approx 128$).

\paragraph{Search.}
Search-after-churn: \VAR{abseil_search_after_churn_mops_large} Mop/s.
Abseil uses scalar binary search within leaves.  The wider leaves
reduce tree height (fewer pointer chases) but increase the number of
comparisons per leaf ($\lceil\log_2 256\rceil = 8$ vs.\ $\lceil\log_2
128\rceil = 7$ for TLX).

\paragraph{TLX vs.\ Abseil.}
On random insert at $N{=}\num{\VAR{largest_n}}$, TLX and Abseil are
within \VAR{btree_insert_gap_pct}\% of each other.  Abseil's wider
leaves trade cheaper outer traversal (fewer levels) for more expensive
in-leaf operations (larger \texttt{memmove}).  The two designs
converge in throughput because the cache miss cost of locating the
target leaf dominates at large $N$.

\subsection{Detailed Comparison: libart (Adaptive Radix Tree)}

ART uses a radix/trie structure with adaptive node types (Node4,
Node16, Node48, Node256) that compact sparse levels.  For 4-byte keys,
the tree has at most 4 levels regardless of $N$.

\paragraph{Insert and delete.}
At $N{=}\num{\VAR{largest_n}}$: \VAR{art_rand_insert_mops_large}
Mop/s (random insert) and \VAR{art_rand_delete_mops_large} Mop/s
(random delete).  ART's $O(k)$ complexity (key length, not tree size)
means insert cost is nearly constant across dataset sizes.  The
scaling ratio from $N{=}\num{\VAR{smallest_n}}$ to
$N{=}\num{\VAR{largest_n}}$ is \VAR{scale_art_rand_insert}$\times$
---the flattest of all structures tested.

\paragraph{Search.}
Search-after-churn: \VAR{art_search_after_churn_mops_large} Mop/s.
ART achieves the highest absolute search throughput because its point
lookups traverse $\le 4$ levels, each requiring a single indexed array
access (no comparison-based search within nodes for Node256).
\emph{However}, the benchmark uses point lookups for ART rather than
predecessor search (which ART does not natively support), so this
comparison is not apples-to-apples with the other structures.

\paragraph{Access pattern interaction.}
ART's per-byte radix decomposition means key distribution matters less
than key length.  The uniform random keys in these benchmarks create
well-distributed tries with few path-compressed nodes.  A workload
with clustered keys sharing long common prefixes would trigger more
path compression and potentially different performance
characteristics.

\paragraph{Memory overhead.}
ART's adaptive node types (4, 16, 48, or 256 children) trade memory
for access speed.  At high occupancy, most internal nodes are Node48
or Node256, using 256--2048\,B per node regardless of actual
fan-out---significantly more memory per key than B-tree or matryoshka
designs.

\subsection{Hardware Counter Comparison}

\BLOCK{if hw_rows}
\begin{table}[H]
\centering
\caption{Hardware counters across all libraries (rand\_insert, $N{=}\num{\VAR{largest_n}}$).}
\label{tab:hw_compare}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Library} & \textbf{Cache Miss} & \textbf{L1d Miss}
  & \textbf{LLC Miss} & \textbf{dTLB Miss} & \textbf{Branch Miss}
  & \textbf{IPC} \\
  & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)}
  & \textbf{(/1K ops)} & \textbf{(\%)} & \\
\midrule
\BLOCK{for row in hw_rows}
\BLOCK{if row.name == 'matryoshka'}
\rowcolor{rowhl}
\BLOCK{endif}
\texttt{\VAR{row.name}} & \VAR{row.cache_miss_pct}
  & \VAR{row.l1d_miss_pct} & \VAR{row.llc_miss_pct}
  & \VAR{row.dtlb_miss_per_1k} & \VAR{row.branch_miss_pct}
  & \VAR{row.ipc} \\
\BLOCK{endfor}
\bottomrule
\end{tabular}
\end{table}

The hardware counters reveal distinct micro-architectural profiles:

\paragraph{dTLB pressure.}
Matryoshka's arena allocator (2\,MiB hugepage-backed regions) yields
the lowest dTLB miss rate (\VAR{matryoshka_dtlb_miss_rate}/\num{1000}
ops).  \texttt{std::set} has the highest
(\VAR{stdset_dtlb_miss_rate}/\num{1000}) because each pointer chase
to a heap-allocated node potentially touches a new 4\,KiB page.
The B-tree libraries (TLX, Abseil) fall between: their wider nodes
reduce the number of distinct pages accessed per operation, but
they use standard \texttt{malloc} without hugepage awareness.
ART's adaptive nodes are heap-allocated but fewer in number than
red-black tree nodes, yielding moderate dTLB pressure.

\paragraph{LLC misses.}
At $N{=}\num{\VAR{largest_n}}$ (random insert), all structures exceed
LLC capacity.  Matryoshka's LLC miss rate is
\VAR{matryoshka_llc_miss_rate}/\num{1000} ops vs.\
\VAR{stdset_llc_miss_rate} for \texttt{std::set}.  Matryoshka
confines each leaf operation to a single 4\,KiB page (or a few
adjacent cache lines for CL sub-node navigation), while red-black
tree operations touch $\log_2 N$ widely-scattered nodes.  The B-tree
libraries achieve comparable LLC rates because their wide leaves
also localise work.

\paragraph{IPC.}
Matryoshka achieves IPC of \VAR{matryoshka_ipc} via SIMD search
(\texttt{\_mm\_cmpgt\_epi32}/\texttt{\_mm\_movemask\_ps}) at every
level, avoiding data-dependent branches.  \texttt{std::set}'s
pointer-chasing and branch-heavy traversal yields the lowest IPC.
ART's indexed array lookups (no comparisons for Node256) can achieve
high IPC on cache-hot paths.

\paragraph{Branch misprediction.}
SIMD mask arithmetic in matryoshka replaces conditional branches,
yielding low branch misprediction rates.  \texttt{std::set} and the
scalar-binary-search B-trees (TLX, Abseil) have higher
misprediction rates because each comparison is a data-dependent
branch.  ART's indexed-lookup approach avoids comparison branches
within nodes but has conditional branches for node type dispatch.
\BLOCK{else}
Hardware counter data was not collected for this run.  Re-run with
\texttt{perf} support enabled (omit \texttt{-{}-no-perf}) to populate
this section.
\BLOCK{endif}

\subsection{Access Pattern Interactions with Data Structure Layout}

The interaction between access pattern and memory layout explains
much of the performance variation:

\paragraph{Sequential vs.\ random insert.}
Sequential insert favours structures with efficient append paths.
All B-tree variants (matryoshka, TLX, Abseil) benefit because new
keys land at the rightmost leaf.  \texttt{std::set} benefits less
because red-black rebalancing is oblivious to key order.  The
throughput ratio (seq/rand) at $N{=}\num{\VAR{largest_n}}$ reveals
how much each structure benefits from locality:
matryoshka \VAR{ratio_mat_stdset_seq_insert}$\times$ vs.\
std::set on sequential, \VAR{ratio_mat_stdset_rand_insert}$\times$
on random.

\paragraph{Delete after bulk-load vs.\ interleaved.}
The \texttt{rand\_delete} workload starts from a bulk-loaded
(optimally packed) tree, giving every structure its best starting
point.  The \texttt{mixed} workload, by contrast, operates on a
tree that is continuously modified, creating internal fragmentation.
Structures that maintain good occupancy under churn (B-trees with
merge/redistribute) degrade less between these workloads than
structures with per-node allocation (\texttt{std::set}).

\paragraph{The 4\,KiB page boundary.}
Matryoshka's 4\,KiB page leaves are sized to match the OS page size,
ensuring that navigating the CL sub-tree within a leaf never crosses a
page boundary.  TLX and Abseil leaves are smaller ($<$1\,KiB), so
multiple leaves may share a page---good for spatial locality of
adjacent leaves, but each leaf may straddle two cache lines for the
\texttt{memmove} operation.  \texttt{std::set} nodes are scattered
across the heap with no page-alignment guarantees.

\paragraph{SIMD and branch prediction.}
Matryoshka's SIMD search produces a bit mask rather than a conditional
branch, making it prediction-friendly.  The sorted-array B-trees
(TLX, Abseil) use scalar binary search with $O(\log B)$
data-dependent branches per leaf, which the branch predictor
struggles with for uniform random keys (50/50 taken probability
at each comparison).

\subsection{Proposed Additional Access Patterns}

Several workloads not currently benchmarked would reveal different
performance relationships:

\begin{description}[style=nextline, nosep, leftmargin=1em]
  \item[Zipfian (skewed) insert]
    A Zipfian distribution concentrates inserts on a small number of
    ``hot'' leaves.  B-tree variants would benefit from cache-hot leaves;
    \texttt{std::set} would benefit from a cache-hot path of recently
    accessed nodes.  Matryoshka's per-page CL sub-tree might show
    more frequent CL splits under concentrated load.

  \item[Range scan after insert]
    Iterate over a range of $k$ keys (e.g.\ $k = 1000$) after building
    the tree.  Matryoshka's linked leaf pages and dense packing should
    excel; \texttt{std::set}'s in-order traversal via parent pointers
    would lag.  This would highlight the spatial locality advantage of
    contiguous leaf storage.

  \item[Interleaved point lookup and insert]
    A read-modify-write pattern (``contains then insert if absent'')
    would test whether search and insert share cache-hot state.
    Matryoshka's search and insert paths share the same CL sub-tree
    navigation code, so a just-searched path remains cache-hot for
    the subsequent insert.

  \item[Large-key workload]
    Keys longer than 4 bytes (e.g.\ 16- or 32-byte strings) would
    stress ART's strength (key-length-dependent, not N-dependent
    traversal) while increasing matryoshka's CL sub-node overhead
    (fewer keys per 64\,B cache line).  B-tree \texttt{memmove}
    cost would grow linearly with key size.

  \item[Delete-heavy with searches (YCSB-D)]
    A workload where the tree shrinks from $N$ to near-empty while
    servicing read queries.  This would stress merge and redistribute
    paths, and test whether search throughput degrades as the tree
    becomes sparsely populated.  Matryoshka's CL sub-node merge and
    page-level redistribute are designed for graceful degradation, but
    extreme sparsity (few keys spread across many pages) could hurt
    cache utilisation.

  \item[Bulk-load comparison]
    Timing the bulk-load operation itself (currently untimed) would
    highlight structural differences: matryoshka distributes keys
    across CL sub-nodes within pages in a single bottom-up pass;
    TLX and Abseil use repeated insertion; \texttt{std::set} has no
    bulk-load optimisation.
\end{description}

\subsection{Overall Assessment}

The matryoshka nesting design achieves competitive insert and delete
throughput while preserving SIMD-accelerated search at every level of
the hierarchy.  Each modification touches a single cache-line sub-node
rather than rebuilding an entire sorted leaf array.  At the largest
dataset size ($N{=}\num{\VAR{largest_n}}$), matryoshka's throughput is
within \VAR{insert_slowdown_factor}$\times$ of the best B-tree
competitor on insert-heavy workloads and
\VAR{delete_slowdown_factor}$\times$ on delete-heavy workloads.

The primary cost of the nesting design is a constant-factor overhead
per modification (CL sub-tree navigation), which the flat sorted-array
B-trees avoid.  This overhead is most visible at small $N$ where the
outer tree is shallow and the per-operation cost is dominated by
in-leaf work.  At large $N$, where the outer tree traversal and cache
misses dominate, the nesting overhead is amortised and matryoshka's
SIMD search and dense page layout become decisive advantages.

%% ═══════════════════════════════════ Recommendations ═══════════════════════
\section{Improvements Since Initial Report}

\subsection{Superpage-Level Nesting (Implemented)}

The nesting now extends to three levels: CL sub-nodes (64\,B) within
4\,KiB pages within 2\,MiB superpages.  Each superpage contains a
B+ tree of page-level sub-nodes, with page-level internal nodes
(681 separator keys, 682 children per 4\,KiB page) routing searches
to up to 510 page leaves.  Maximum capacity per superpage:
$510 \times 855 \approx 436\text{K}$ keys.  This confines most
operations to a single TLB entry and reduces outer-tree height.
Enable via \texttt{mt\_hierarchy\_init\_superpage}.

\subsection{Wider SIMD: AVX2 and AVX-512 (Implemented)}

Compile-time SIMD width selection via \texttt{-DMT\_SIMD=avx2} or
\texttt{-DMT\_SIMD=avx512}.  AVX2 (256-bit) processes 8 keys per
comparison in CL leaf predecessor search, CL internal search, and
outer internal node search.  AVX-512 (512-bit) processes 16 keys
per comparison using masked operations.  Unaligned loads handle the
4-byte header offset within CL sub-nodes.  SSE2 (128-bit, 4 keys)
remains the baseline fallback.

\subsection{Batch Insert and Delete API (Implemented)}

\texttt{matryoshka\_insert\_batch(tree, keys, n)} and
\texttt{matryoshka\_delete\_batch(tree, keys, n)} sort incoming keys,
group them by target leaf, and amortise outer-tree traversal across
each group.  On page-full or underflow, the path is re-navigated for
remaining keys.  Both functions work with page leaves and superpages.

\subsection{Future: Variable-Length Keys}

The current 4-byte \texttt{int32\_t} key format could be extended to
variable-length keys by storing key offsets or using indirection within
CL sub-nodes.  This would broaden applicability at the cost of some
cache efficiency.

%% ═══════════════════════════════════ References ════════════════════════════
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{enumerate}[label={[\arabic*]}, nosep, leftmargin=2em]
  \item \label{ref:fast}
        C.~Kim, J.~Chhugani, N.~Satish, E.~Sedlar, A.~D.~Nguyen,
        T.~Kaldewey, V.~W.~Lee, S.~A.~Brandt, and P.~Dubey.
        \emph{FAST: Fast Architecture Sensitive Tree Search on Modern
        CPUs and GPUs.}  SIGMOD~'10, pp.~339--350, 2010.
  \item \label{ref:btree}
        R.~Bayer and E.~McCreight.
        \emph{Organization and Maintenance of Large Ordered Indexes.}
        Acta Informatica, 1(3):173--189, 1972.
  \item \label{ref:art}
        V.~Leis, A.~Kemper, and T.~Neumann.
        \emph{The Adaptive Radix Tree: ARTful Indexing for Main-Memory
        Databases.}  ICDE~'13, pp.~38--49, 2013.
  \item \label{ref:cssbtree}
        J.~Rao and K.~A.~Ross.
        \emph{Making B+-Trees Cache Conscious in Main Memory.}
        SIGMOD~'00, pp.~475--486, 2000.
\end{enumerate}

\end{document}
